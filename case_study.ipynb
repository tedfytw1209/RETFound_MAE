{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b38c954-c9b3-41f2-808b-c16d81e7a700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set environment variable to avoid symbolic tracing issues\n",
    "import os\n",
    "os.environ['TIMM_FUSED_ATTN'] = '0'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "from pytorch_grad_cam import run_dff_on_image, GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from typing import List, Callable, Optional\n",
    "\n",
    "# Import XAI methods\n",
    "from baselines.GradCAM_v2 import PytorchCAM\n",
    "from baselines.RISE import RISEBatch\n",
    "from baselines.Attention import Attention_Map\n",
    "from baselines.CRP_LXT import CRP_LXT\n",
    "from pytorch_grad_cam import GradCAM, ScoreCAM\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "088415c1-dba0-4047-aee5-53bc1fabac72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    ViTImageProcessor, ViTForImageClassification,\n",
    "    AutoImageProcessor, EfficientNetForImageClassification,\n",
    "    ResNetForImageClassification, AutoModel\n",
    ")\n",
    "import models_vit as models\n",
    "from util.datasets import TransformWrapper\n",
    "import timm\n",
    "\n",
    "#get model\n",
    "def get_model(task,model,input_size,nb_classes):\n",
    "    if 'ADCon' in task:\n",
    "        id2label = {0: \"control\", 1: \"ad\"}\n",
    "        label2id = {v: k for k, v in id2label.items()}\n",
    "    else:\n",
    "        id2label = {i: f\"class_{i}\" for i in range(nb_classes)}\n",
    "        label2id = {v: k for k, v in id2label.items()}\n",
    "    processor = None\n",
    "    if 'RETFound_mae' in model:\n",
    "        model = models.__dict__['RETFound_mae'](\n",
    "        img_size=input_size,\n",
    "        num_classes=nb_classes,\n",
    "        drop_path_rate=0.2,\n",
    "        global_pool=True,\n",
    "    )\n",
    "    elif 'vit-base-patch16-224' in model:\n",
    "        # ViT-base-patch16-224 preprocessor\n",
    "        model_ = 'google/vit-base-patch16-224'\n",
    "        processor = TransformWrapper(ViTImageProcessor.from_pretrained(model_))\n",
    "        model = ViTForImageClassification.from_pretrained(\n",
    "            model_,\n",
    "            image_size=input_size, #Not in tianhao code, default 224\n",
    "            num_labels=nb_classes,\n",
    "            hidden_dropout_prob=0.0, #Not in tianhao code, default 0.0\n",
    "            attention_probs_dropout_prob=0.0, #Not in tianhao code, default 0.0\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "            ignore_mismatched_sizes=True,\n",
    "            attn_implementation=\"eager\",      # ‚Üê key line\n",
    "        )\n",
    "        model.config.return_dict = True\n",
    "        model.config.output_attentions = True\n",
    "    elif 'timm_efficientnet-b4' in model:\n",
    "        model = timm.create_model('efficientnet_b4', pretrained=True, num_classes=nb_classes)\n",
    "        processor  = transforms.Compose([\n",
    "            transforms.Resize((380,380)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "        ])\n",
    "    elif 'ResNet-50' in model:\n",
    "        model_name = 'microsoft/resnet-50'\n",
    "        processor = TransformWrapper(AutoImageProcessor.from_pretrained(model_name))\n",
    "        model = ResNetForImageClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=nb_classes,\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        model.config.return_dict = True\n",
    "        model.config.output_attentions = True\n",
    "\n",
    "    return model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b94f0a85-078e-4a03-a4f7-57bf8778c49c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# task and dataset\n",
    "#Task_list = ['ADCon','DME']\n",
    "Task_list = ['DME']\n",
    "dataset_fname = 'sampled_labels01.csv'\n",
    "dataset_dir = '/blue/ruogu.fang/tienyuchang/OCT_EDA'\n",
    "img_p_fmt = \"label_%d/%s\" #label index and oct_img name\n",
    "\n",
    "# model\n",
    "Model_root = \"/blue/ruogu.fang/tienyuchang/RETFound_MAE/output_dir\"\n",
    "Model_fname = \"checkpoint-best.pth\"\n",
    "Model_list = ['ResNet-50', 'timm_efficientnet-b4', 'vit-base-patch16-224', 'RETFound_mae']\n",
    "ADCon_finetuned = [\n",
    "    \"ad_control_detect_data-IRB2024v5_ADCON_DL_data-all-resnet-50-OCT-defaulteval---bal_sampler-/\",\n",
    "    \"ad_control_detect_data-IRB2024v5_ADCON_DL_data-all-timm_efficientnet-b4-OCT-defaulteval---bal_sampler-/\",\n",
    "    \"ad_control_detect_data-IRB2024v5_ADCON_DL_data-all-vit-base-patch16-224-OCT-defaulteval---bal_sampler-/\",\n",
    "    \"ad_control_detect_data-IRB2024v5_ADCON_DL_data-all-RETFound_mae-OCT-defaulteval---bal_sampler-/\"\n",
    "]\n",
    "DME_finetuned = [\n",
    "    \"DME_binary_all_split-IRB2024_v5-all-microsoft/resnet-50-OCT-bs16ep50lr5e-4optadamw-defaulteval-trsub0--/\",\n",
    "    \"DME_binary_all_split-IRB2024_v5-all-timm_efficientnet-b4-OCT-bs16ep50lr5e-4optadamw-defaulteval-trsub0--/\",\n",
    "    \"DME_binary_all_split-IRB2024_v5-all-google/vit-base-patch16-224-in21k-OCT-bs16ep50lr5e-4optadamw-defaulteval-trsub0--/\",\n",
    "    \"DME_binary_all_split-IRB2024_v5-all-RETFound_mae_natureOCT-OCT-bs16ep50lr5e-4optadamw-roc_auceval-trsub0--/\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3009084",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "<PIL.Image.Image image mode=RGB size=512x496 at 0x14B9BE7AD910>\n",
      "[0, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Data loading and preprocessing functions\n",
    "def load_sample_data(task, num_sample=-1):\n",
    "    \"\"\"Load sample images for a given task\"\"\"\n",
    "    df = pd.read_csv(os.path.join(dataset_dir, \"%s_sampled\"%task, dataset_fname))\n",
    "    task_df = df[df['label'].isin([0, 1])]  # Adjust based on actual DME labels\n",
    "    # Sample random images\n",
    "    if num_sample > 0:\n",
    "        task_df = task_df.sample(n=num_sample, random_state=42).reset_index(drop=True)\n",
    "    else:\n",
    "        task_df = task_df.reset_index(drop=True)\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    filenames = []\n",
    "    \n",
    "    for _, row in task_df.iterrows():\n",
    "        # Extract just the filename from oct_img\n",
    "        filename = os.path.basename(row['OCT']) if isinstance(row['OCT'], str) else row['OCT']\n",
    "        img_path = os.path.join(dataset_dir, \"%s_sampled\"%task, img_p_fmt % (row['label'], filename))\n",
    "        if os.path.exists(img_path):\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                images.append(img)\n",
    "                labels.append(row['label'])\n",
    "                # Store filename without extension for directory naming\n",
    "                image_name = os.path.splitext(filename)[0]\n",
    "                filenames.append(image_name)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_path}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return images, labels, filenames\n",
    "\n",
    "def preprocess_image(image, processor=None, input_size=224, device=None, dtype=torch.float32):\n",
    "    assert isinstance(image, Image.Image), f\"expect PIL.Image, got {type(image)}\"\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if processor is not None:\n",
    "        # A) ÂÖàÂ∞ùËØï‚ÄúÁõ¥Êé•ÂèØË∞ÉÁî®‚ÄùÂΩ¢ÂºèÔºàÂ§öÊï∞ timm/torchvision transformÔºâ\n",
    "        try:\n",
    "            out = processor(image)\n",
    "            if isinstance(out, torch.Tensor):\n",
    "                x = out\n",
    "                if x.ndim == 3:  # [C,H,W] -> [1,C,H,W]\n",
    "                    x = x.unsqueeze(0)\n",
    "                return x.to(device=device, dtype=dtype)\n",
    "            if isinstance(out, dict) and \"pixel_values\" in out:\n",
    "                x = out[\"pixel_values\"]\n",
    "                if isinstance(x, np.ndarray):\n",
    "                    x = torch.from_numpy(x)\n",
    "                if x.ndim == 3:\n",
    "                    x = x.unsqueeze(0)\n",
    "                return x.to(device=device, dtype=dtype)\n",
    "        except TypeError:\n",
    "            pass\n",
    "\n",
    "        # B) ÂÜçÂ∞ùËØï HuggingFace È£éÊ†ºÔºà‰∏ç‰ΩøÁî® images= ÂÖ≥ÈîÆÂ≠óÔºâ\n",
    "        try:\n",
    "            out = processor(image, return_tensors=\"pt\")\n",
    "            if isinstance(out, dict) and \"pixel_values\" in out:\n",
    "                x = out[\"pixel_values\"]  # [1,3,H,W]\n",
    "                return x.to(device=device, dtype=dtype)\n",
    "            if isinstance(out, torch.Tensor):\n",
    "                x = out\n",
    "                if x.ndim == 3:\n",
    "                    x = x.unsqueeze(0)\n",
    "                return x.to(device=device, dtype=dtype)\n",
    "        except TypeError:\n",
    "            pass\n",
    "\n",
    "        # C) Êüê‰∫õÂÆûÁé∞‰ªÖÊé•ÂèóÂàóË°®\n",
    "        for attempt in (lambda: processor([image], return_tensors=\"pt\"),\n",
    "                        lambda: processor([image])):\n",
    "            try:\n",
    "                out = attempt()\n",
    "                if isinstance(out, dict) and \"pixel_values\" in out:\n",
    "                    x = out[\"pixel_values\"]\n",
    "                    if isinstance(x, np.ndarray):\n",
    "                        x = torch.from_numpy(x)\n",
    "                    return x.to(device=device, dtype=dtype)\n",
    "                if isinstance(out, torch.Tensor):\n",
    "                    x = out\n",
    "                    if x.ndim == 3:\n",
    "                        x = x.unsqueeze(0)\n",
    "                    return x.to(device=device, dtype=dtype)\n",
    "            except TypeError:\n",
    "                pass\n",
    "\n",
    "    # D) ÂõûÈÄÄÔºöÊ†áÂáÜ ImageNet È¢ÑÂ§ÑÁêÜ\n",
    "    fallback = transforms.Compose([\n",
    "        transforms.Resize((input_size, input_size)),\n",
    "        transforms.ToTensor(),  # [0,1]\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    x = fallback(image)            # [3,H,W]\n",
    "    x = x.unsqueeze(0)             # [1,3,H,W]\n",
    "    return x.to(device=device, dtype=dtype)\n",
    "\n",
    "#test dataset\n",
    "dme_imgs, dme_labels, dme_img_names = load_sample_data('DME',5)\n",
    "print(len(dme_imgs))\n",
    "print(dme_imgs[0])\n",
    "print(dme_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcdfdc79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel_list = []\\nfor model_name in Model_list:\\n    model, processor = load_trained_model('DME', model_name, 224)\\n    print(type(model))\\n    model_list.append(model)\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load trained models function\n",
    "def load_trained_model(task, model_name, input_size=224, nb_classes=2):\n",
    "    \"\"\"Load a trained model for a specific task\"\"\"\n",
    "    model, processor = get_model(task, model_name, input_size, nb_classes)\n",
    "    \n",
    "    # Load model weights based on task and model\n",
    "    if task == 'ADCon':\n",
    "        model_paths = ADCon_finetuned\n",
    "    elif task == 'DME':\n",
    "        model_paths = DME_finetuned\n",
    "    else:\n",
    "        print(f\"Unknown task: {task}\")\n",
    "        model.eval()\n",
    "        return model, processor\n",
    "    \n",
    "    model_idx = Model_list.index(model_name)\n",
    "    model_dir = model_paths[model_idx]\n",
    "    model_path = os.path.join(Model_root, model_dir, Model_fname)\n",
    "    \n",
    "    # Load finetuned model if specified (following main_XAI_evaluation.py pattern)\n",
    "    if model_path and model_path != '':\n",
    "        if os.path.exists(model_path):\n",
    "            try:\n",
    "                # Load checkpoint\n",
    "                if model_path.startswith('https'):\n",
    "                    checkpoint = torch.hub.load_state_dict_from_url(\n",
    "                        model_path, map_location='cpu', check_hash=True)\n",
    "                else:\n",
    "                    checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)\n",
    "                \n",
    "                # Extract model state dict\n",
    "                if 'model' in checkpoint:\n",
    "                    checkpoint_model = checkpoint['model']\n",
    "                else:\n",
    "                    checkpoint_model = checkpoint\n",
    "                \n",
    "                # Load with strict=False to handle potential mismatches\n",
    "                model.load_state_dict(checkpoint_model, strict=False)\n",
    "                print(f\"Resume checkpoint {model_path} for {model_name} on {task}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading model {model_name} for {task}: {e}\")\n",
    "                print(\"Using pretrained weights instead\")\n",
    "        else:\n",
    "            print(f\"Model path not found: {model_path}\")\n",
    "            print(f\"Using pretrained weights for {model_name} on {task}\")\n",
    "    else:\n",
    "        print(f\"No checkpoint specified for {model_name} on {task}, using pretrained weights\")\n",
    "    \n",
    "    model.eval()\n",
    "    return model, processor\n",
    "\n",
    "#test\n",
    "'''\n",
    "model_list = []\n",
    "for model_name in Model_list:\n",
    "    model, processor = load_trained_model('DME', model_name, 224)\n",
    "    print(type(model))\n",
    "    model_list.append(model)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d00dd935",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#test\\nfor model_name in Model_list:\\n    model, processor = load_trained_model('DME', model_name, 224)\\n    XAIGenerator(model, model_name)\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XAI Methods Implementation\n",
    "class XAIGenerator:\n",
    "    def __init__(self, model, model_name, input_size=224):\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.input_size = input_size\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # Initialize XAI methods\n",
    "        self.init_xai_methods()\n",
    "    \n",
    "    def get_model_specific_config(self):\n",
    "        \"\"\"Get model-specific configuration for XAI methods\"\"\"\n",
    "        config = {\n",
    "            'patch_size': 14,\n",
    "            'gpu_batch': 1,\n",
    "            'attention_layers': 12\n",
    "        }\n",
    "        \n",
    "        # Model-specific configurations\n",
    "        if 'resnet' in self.model_name.lower():\n",
    "            config.update({\n",
    "                'patch_size': 7,  # ResNet has different spatial resolution\n",
    "                'gpu_batch': 1,  # ResNet can handle larger batches\n",
    "            })\n",
    "        elif 'efficientnet' in self.model_name.lower():\n",
    "            config.update({\n",
    "                'patch_size': 7,  # EfficientNet spatial resolution\n",
    "                'gpu_batch': 1,\n",
    "            })\n",
    "        elif 'vit' in self.model_name.lower():\n",
    "            config.update({\n",
    "                'patch_size': 16,  # ViT patch size\n",
    "                'gpu_batch': 1,\n",
    "                'attention_layers': 12,  # Standard ViT-Base layers\n",
    "            })\n",
    "        elif 'retfound' in self.model_name.lower():\n",
    "            config.update({\n",
    "                'patch_size': 16,  # RETFound uses ViT architecture\n",
    "                'gpu_batch': 1,\n",
    "                'attention_layers': 12,\n",
    "            })\n",
    "        \n",
    "        return config\n",
    "    \n",
    "    def init_xai_methods(self):\n",
    "        \"\"\"Initialize all XAI methods with model-specific configurations\"\"\"\n",
    "        config = self.get_model_specific_config()\n",
    "        # GradCAM with model-specific config\n",
    "        self.gradcam = PytorchCAM(\n",
    "            self.model, \n",
    "            self.model_name, \n",
    "            self.input_size, \n",
    "            patch_size=config['patch_size'],\n",
    "            method=GradCAM\n",
    "        )\n",
    "        print(f\"‚úì GradCAM initialized for {self.model_name} (patch_size: {config['patch_size']})\")\n",
    "        \n",
    "        # ScoreCAM with model-specific config\n",
    "        self.scorecam = PytorchCAM(\n",
    "            self.model, \n",
    "            self.model_name, \n",
    "            self.input_size, \n",
    "            patch_size=config['patch_size'],\n",
    "            method=ScoreCAM\n",
    "        )\n",
    "        print(f\"‚úì ScoreCAM initialized for {self.model_name} (patch_size: {config['patch_size']})\")\n",
    "        \n",
    "        # RISE with model-specific batch size\n",
    "        # Reduce batch for memory-heavy models\n",
    "        rise_batch = config['gpu_batch']\n",
    "        self.rise = RISEBatch(\n",
    "            self.model, \n",
    "            input_size=(self.input_size, self.input_size), \n",
    "            gpu_batch=rise_batch,\n",
    "            N=100\n",
    "        )\n",
    "        print(f\"‚úì RISE initialized for {self.model_name} (gpu_batch: {rise_batch})\")\n",
    "        \n",
    "        # CRP_LXT with model-specific batch size\n",
    "        # Reduce batch for memory-heavy models\n",
    "        batch = config['gpu_batch']\n",
    "        self.crp_lxt = CRP_LXT(\n",
    "            self.model, \n",
    "            self.model_name,\n",
    "            img_size=(self.input_size, self.input_size)\n",
    "        )\n",
    "        print(f\"‚úì CRP_LXT initialized for {self.model_name} (gpu_batch: {rise_batch})\")\n",
    "        \n",
    "        try:\n",
    "            # Attention Maps (only for ViT-based models)\n",
    "            if 'vit' in self.model_name.lower() or 'retfound' in self.model_name.lower():\n",
    "                self.attention = Attention_Map(\n",
    "                    self.model, \n",
    "                    self.model_name, \n",
    "                    self.input_size, \n",
    "                    N=config['attention_layers'],\n",
    "                    use_rollout=True,\n",
    "                    print_layers=False  # Disable layer printing to avoid issues\n",
    "                )\n",
    "                print(f\"‚úì Attention initialized for {self.model_name} (layers: {config['attention_layers']})\")\n",
    "            else:\n",
    "                self.attention = None\n",
    "                print(f\"‚ö† Attention skipped for {self.model_name} (not a transformer model)\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Failed to initialize Attention for {self.model_name}: {e}\")\n",
    "            print(f\"  Note: This may be due to symbolic tracing issues with the model architecture\")\n",
    "            \n",
    "            # Try alternative attention extraction for RETFound\n",
    "            if 'retfound' in self.model_name.lower():\n",
    "                try:\n",
    "                    print(f\"  üîÑ Trying alternative attention extraction for RETFound...\")\n",
    "                    self.attention = self._create_simple_attention_extractor()\n",
    "                    print(f\"‚úì Alternative attention method initialized for {self.model_name}\")\n",
    "                except Exception as e2:\n",
    "                    print(f\"‚úó Alternative attention method also failed: {e2}\")\n",
    "                    self.attention = None\n",
    "            else:\n",
    "                self.attention = None\n",
    "        \n",
    "        # LRP (requires special model implementation)\n",
    "        self.lrp = None  # Will implement if model supports it\n",
    "    \n",
    "    def generate_gradcam(self, image_tensor, target_class=None):\n",
    "        \"\"\"Generate GradCAM heatmap\"\"\"\n",
    "        if self.gradcam is None:\n",
    "            return None\n",
    "        image_tensor = image_tensor.to(self.device)\n",
    "        if target_class is None:\n",
    "            # Get predicted class\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(image_tensor)\n",
    "                target_class = outputs.argmax(dim=1).item()\n",
    "\n",
    "        targets = [ClassifierOutputTarget(target_class)]\n",
    "        heatmap = self.gradcam(image_tensor, targets)\n",
    "        return heatmap[0] if len(heatmap) > 0 else None\n",
    "    \n",
    "    def generate_scorecam(self, image_tensor, target_class=None):\n",
    "        \"\"\"Generate ScoreCAM heatmap\"\"\"\n",
    "        if self.scorecam is None:\n",
    "            return None\n",
    "        image_tensor = image_tensor.to(self.device)\n",
    "        if target_class is None:\n",
    "            # Get predicted class\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(image_tensor)\n",
    "                target_class = outputs.argmax(dim=1).item()\n",
    "\n",
    "        targets = [ClassifierOutputTarget(target_class)]\n",
    "        heatmap = self.scorecam(image_tensor, targets)\n",
    "        return heatmap[0] if len(heatmap) > 0 else None\n",
    "    \n",
    "    def generate_rise(self, image_tensor, target_class=None):\n",
    "        \"\"\"Generate RISE heatmap\"\"\"\n",
    "        if self.rise is None:\n",
    "            return None\n",
    "        image_tensor = image_tensor.to(self.device)\n",
    "\n",
    "        if target_class is None:\n",
    "            # Get predicted class\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(image_tensor)\n",
    "                target_class = outputs.argmax(dim=1).item()\n",
    "\n",
    "        heatmaps = self.rise(image_tensor)\n",
    "        return heatmaps[0, target_class] if heatmaps is not None else None\n",
    "    \n",
    "    def generate_crp_lxt(self, image_tensor, target_class=None):\n",
    "        \"\"\"Generate CRP_LXT heatmap\"\"\"\n",
    "        if self.crp_lxt is None:\n",
    "            return None\n",
    "        image_tensor = image_tensor.to(self.device)\n",
    "\n",
    "        if target_class is None:\n",
    "            # Get predicted class\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(image_tensor)\n",
    "                target_class = outputs.argmax(dim=1).item()\n",
    "\n",
    "        heatmaps = self.crp_lxt(image_tensor, target_class)\n",
    "        return heatmaps\n",
    "    \n",
    "    def generate_attention(self, image_tensor):\n",
    "        \"\"\"Generate Attention heatmap\"\"\"\n",
    "        if self.attention is None:\n",
    "            return None\n",
    "        image_tensor = image_tensor.to(self.device)\n",
    "        attention_map = self.attention(image_tensor)\n",
    "        return attention_map[0] if attention_map is not None else None\n",
    "    \n",
    "    def generate_all_heatmaps(self, image_tensor, target_class=None):\n",
    "        \"\"\"Generate all available heatmaps for an image\"\"\"\n",
    "        heatmaps = {}\n",
    "        \n",
    "        # GradCAM\n",
    "        gradcam_map = self.generate_gradcam(image_tensor, target_class)\n",
    "        if gradcam_map is not None:\n",
    "            heatmaps['GradCAM'] = gradcam_map\n",
    "        \n",
    "        # ScoreCAM\n",
    "        scorecam_map = self.generate_scorecam(image_tensor, target_class)\n",
    "        if scorecam_map is not None:\n",
    "            heatmaps['ScoreCAM'] = scorecam_map\n",
    "        \n",
    "        # RISE\n",
    "        rise_map = self.generate_rise(image_tensor, target_class)\n",
    "        if rise_map is not None:\n",
    "            heatmaps['RISE'] = rise_map\n",
    "        \n",
    "        # Attention\n",
    "        attention_map = self.generate_attention(image_tensor)\n",
    "        if attention_map is not None:\n",
    "            heatmaps['Attention'] = attention_map\n",
    "            \n",
    "        # CRP_LXT\n",
    "        crp_lxt_map = self.generate_crp_lxt(image_tensor, target_class)\n",
    "        if crp_lxt_map is not None:\n",
    "            heatmaps['CRP_LXT'] = crp_lxt_map\n",
    "        \n",
    "        return heatmaps\n",
    "    \n",
    "    def _create_simple_attention_extractor(self):\n",
    "        \"\"\"Create a simple attention extractor that doesn't use symbolic tracing\"\"\"\n",
    "        class SimpleAttentionExtractor:\n",
    "            def __init__(self, model, model_name, input_size):\n",
    "                self.model = model\n",
    "                self.model_name = model_name\n",
    "                self.input_size = input_size\n",
    "                self.attention_maps = []\n",
    "                \n",
    "                # Register hooks to capture attention weights\n",
    "                self.hooks = []\n",
    "                if hasattr(model, 'blocks'):\n",
    "                    for i, block in enumerate(model.blocks):\n",
    "                        if hasattr(block, 'attn'):\n",
    "                            hook = block.attn.register_forward_hook(self._attention_hook)\n",
    "                            self.hooks.append(hook)\n",
    "            \n",
    "            def _attention_hook(self, module, input, output):\n",
    "                \"\"\"Hook to capture attention weights\"\"\"\n",
    "                # Try to get attention weights from different possible attributes\n",
    "                if hasattr(module, 'attention_probs'):\n",
    "                    self.attention_maps.append(module.attention_probs.detach())\n",
    "                elif hasattr(module, 'attn_weights'):\n",
    "                    self.attention_maps.append(module.attn_weights.detach())\n",
    "                elif len(output) > 1 and isinstance(output[1], torch.Tensor):\n",
    "                    # Some models return attention as second output\n",
    "                    self.attention_maps.append(output[1].detach())\n",
    "            \n",
    "            def __call__(self, x):\n",
    "                \"\"\"Extract attention maps\"\"\"\n",
    "                self.attention_maps = []\n",
    "                \n",
    "                # Forward pass to trigger hooks\n",
    "                with torch.no_grad():\n",
    "                    _ = self.model(x)\n",
    "                \n",
    "                if len(self.attention_maps) > 0:\n",
    "                    try:\n",
    "                        # Use the last attention layer\n",
    "                        last_attention = self.attention_maps[-1]  # Shape: (B, num_heads, num_tokens, num_tokens)\n",
    "                        \n",
    "                        # Average over heads and extract CLS attention to patches\n",
    "                        if len(last_attention.shape) == 4:  # (B, num_heads, num_tokens, num_tokens)\n",
    "                            cls_attention = last_attention.mean(dim=1)[:, 0, 1:]  # (B, num_patches)\n",
    "                        else:\n",
    "                            # Fallback for different attention shapes\n",
    "                            cls_attention = last_attention[:, 0, 1:] if last_attention.shape[1] > 1 else last_attention[:, 0]\n",
    "                        \n",
    "                        # Reshape to spatial dimensions\n",
    "                        B = cls_attention.shape[0]\n",
    "                        num_patches = cls_attention.shape[1]\n",
    "                        patch_size = int(num_patches ** 0.5)\n",
    "                        \n",
    "                        if patch_size * patch_size == num_patches:\n",
    "                            attention_maps = cls_attention.reshape(B, patch_size, patch_size)\n",
    "                            \n",
    "                            # Resize to input size\n",
    "                            attention_maps_resized = torch.nn.functional.interpolate(\n",
    "                                attention_maps.unsqueeze(1), \n",
    "                                size=(self.input_size, self.input_size), \n",
    "                                mode='bilinear', \n",
    "                                align_corners=False\n",
    "                            ).squeeze(1)\n",
    "                            \n",
    "                            return attention_maps_resized.cpu().numpy()\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing attention maps: {e}\")\n",
    "                \n",
    "                # Fallback: return zeros\n",
    "                return np.zeros((x.shape[0], self.input_size, self.input_size))\n",
    "            \n",
    "            def cleanup(self):\n",
    "                \"\"\"Remove hooks\"\"\"\n",
    "                for hook in self.hooks:\n",
    "                    hook.remove()\n",
    "        \n",
    "        return SimpleAttentionExtractor(self.model, self.model_name, self.input_size)\n",
    "'''\n",
    "#test\n",
    "for model_name in Model_list:\n",
    "    model, processor = load_trained_model('DME', model_name, 224)\n",
    "    XAIGenerator(model, model_name)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "063fabda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualization functions\n",
    "def normalize_heatmap(heatmap):\n",
    "    \"\"\"Normalize heatmap to 0-1 range\"\"\"\n",
    "    if heatmap is None:\n",
    "        return None\n",
    "    \n",
    "    heatmap = np.array(heatmap)\n",
    "    if heatmap.max() == heatmap.min():\n",
    "        return np.zeros_like(heatmap)\n",
    "    \n",
    "    return (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
    "\n",
    "def overlay_heatmap_on_image(image, heatmap, alpha=0.4, colormap='jet'):\n",
    "    \"\"\"Overlay heatmap on original image\"\"\"\n",
    "    if heatmap is None:\n",
    "        return np.array(image)\n",
    "    \n",
    "    # Normalize heatmap\n",
    "    heatmap_norm = normalize_heatmap(heatmap)\n",
    "    \n",
    "    # Resize heatmap to match image size\n",
    "    if isinstance(image, Image.Image):\n",
    "        image_array = np.array(image)\n",
    "        image_size = image.size\n",
    "    else:\n",
    "        image_array = image\n",
    "        image_size = (image.shape[1], image.shape[0])\n",
    "    \n",
    "    heatmap_resized = cv2.resize(heatmap_norm, image_size)\n",
    "    \n",
    "    # Apply colormap\n",
    "    cmap = plt.get_cmap(colormap)\n",
    "    heatmap_colored = cmap(heatmap_resized)[:, :, :3]  # Remove alpha channel\n",
    "    \n",
    "    # Normalize image\n",
    "    image_norm = image_array.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Overlay\n",
    "    overlay = alpha * heatmap_colored + (1 - alpha) * image_norm\n",
    "    overlay = np.clip(overlay, 0, 1)\n",
    "    \n",
    "    return (overlay * 255).astype(np.uint8)\n",
    "\n",
    "def create_heatmap_grid(results_dict, task, num_samples, save_path=None):\n",
    "    \"\"\"Create a comprehensive grid visualization of all heatmaps\"\"\"\n",
    "    models = Model_list\n",
    "    xai_methods = ['GradCAM', 'ScoreCAM', 'RISE', 'Attention', 'CRP_LXT']\n",
    "    #xai_methods = ['GradCAM', 'ScoreCAM', 'Attention']\n",
    "    \n",
    "    # Calculate grid dimensions\n",
    "    n_models = len(models)\n",
    "    n_methods = len(xai_methods)\n",
    "    \n",
    "    for sample_idx in range(num_samples):\n",
    "        # Create figure\n",
    "        fig_width = max(20, n_methods * 4)\n",
    "        fig_height = max(15, n_models * 3)\n",
    "        fig, axes = plt.subplots(\n",
    "            n_models, \n",
    "            n_methods + 1,  # +1 for original image\n",
    "            figsize=(fig_width, fig_height)\n",
    "        )\n",
    "        if n_models == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        elif len(axes.shape) == 1:\n",
    "            axes = axes.reshape(-1, 1)\n",
    "        for model_idx, model in enumerate(models):\n",
    "            if model not in results_dict[task]:\n",
    "                continue\n",
    "            model_results = results_dict[task][model]\n",
    "            images = model_results['images']\n",
    "            labels = model_results['labels']\n",
    "            heatmaps = model_results['heatmaps']\n",
    "            \n",
    "            image = images[sample_idx]\n",
    "            label = labels[sample_idx]\n",
    "            sample_heatmaps = heatmaps[sample_idx]\n",
    "\n",
    "            # Original image\n",
    "            axes[model_idx, 0].imshow(image)\n",
    "            axes[model_idx, 0].set_title(f'{task}-{model}\\nSample {sample_idx+1} (Label: {label})')\n",
    "            axes[model_idx, 0].axis('off')\n",
    "            # Heatmaps\n",
    "            for method_idx, method in enumerate(xai_methods):\n",
    "                col_idx = method_idx + 1\n",
    "                \n",
    "                if method in sample_heatmaps and sample_heatmaps[method] is not None:\n",
    "                    overlay = overlay_heatmap_on_image(image, sample_heatmaps[method])\n",
    "                    axes[model_idx, col_idx].imshow(overlay)\n",
    "                    axes[model_idx, col_idx].set_title(f'{method}')\n",
    "                else:\n",
    "                    axes[model_idx, col_idx].text(0.5, 0.5, 'N/A', \n",
    "                                                ha='center', va='center', \n",
    "                                                transform=axes[model_idx, col_idx].transAxes)\n",
    "                    axes[model_idx, col_idx].set_title(f'{method}')\n",
    "                axes[model_idx, col_idx].axis('off')\n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            sample_path = save_path.parent / f\"{save_path.stem}_{sample_idx}{save_path.suffix}\"\n",
    "            plt.savefig(sample_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Heatmap grid saved to {save_path}\")\n",
    "        plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "596566fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Updated function with new directory structure for heatmap saving\n",
    "def generate_comprehensive_heatmaps_v2(num_samples=3,task_list=Task_list,model_list=Model_list):\n",
    "    \"\"\"Generate heatmaps for all task-model combinations with new directory structure\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    input_size = 224\n",
    "    \n",
    "    print(\"Starting comprehensive heatmap generation...\")\n",
    "    print(f\"Tasks: {task_list}\")\n",
    "    print(f\"Models: {model_list}\")\n",
    "    print(f\"Samples per task: {num_samples}\")\n",
    "    \n",
    "    for task in task_list:\n",
    "        print(f\"\\n=== Processing Task: {task} ===\")\n",
    "        results[task] = {}\n",
    "        \n",
    "        # Load sample data for this task (now returns filenames too)\n",
    "        try:\n",
    "            images, labels, filenames = load_sample_data(task, num_samples)\n",
    "            print(f\"Loaded {len(images)} images for {task}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data for {task}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        for model_name in model_list:\n",
    "            print(f\"\\n--- Processing Model: {model_name} ---\")\n",
    "            # Load trained model\n",
    "            model, processor = load_trained_model(task, model_name, input_size)\n",
    "            # Initialize XAI generator\n",
    "            xai_generator = XAIGenerator(model, model_name, input_size)\n",
    "            # Store results for this model\n",
    "            results[task][model_name] = {\n",
    "                'images': images,\n",
    "                'labels': labels,\n",
    "                'heatmaps': []\n",
    "            }\n",
    "            # Process each image with filename\n",
    "            for idx, (image, label, filename) in enumerate(zip(images, labels, filenames)):\n",
    "                print(f\"Processing image {idx+1}/{len(images)} (Label: {label}, File: {filename})\")\n",
    "                # Preprocess image\n",
    "                image_tensor = preprocess_image(image, processor, input_size)\n",
    "                # Generate all heatmaps for this image\n",
    "                heatmaps = xai_generator.generate_all_heatmaps(image_tensor, target_class=label)\n",
    "                results[task][model_name]['heatmaps'].append(heatmaps)\n",
    "                for xai_name, heatmap in heatmaps.items():\n",
    "                    overlay = overlay_heatmap_on_image(image, heatmap)\n",
    "                    # overlay is np.uint8 HxWx3 per implementation\n",
    "                    # Create directory structure: ./heatmap_results/<task_name>/<label_idx>/<image_name>/<baselinemodel>/<XAI>.jpg\n",
    "                    save_dir = Path(\"./heatmap_results\") / task / str(label) / filename / model_name\n",
    "                    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "                    out_path = save_dir / f\"{xai_name}.jpg\"\n",
    "                    try:\n",
    "                        Image.fromarray(overlay).save(out_path, format='JPEG', quality=95)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed to save {out_path}: {e}\")\n",
    "            print(f\"Completed {model_name} for {task}\")\n",
    "            #delete after finish\n",
    "            del xai_generator\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cc020db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing New Heatmap Generation Function ===\n",
      "This will save heatmaps in the structure: ./heatmap_results/<task_name>/<label_idx>/<image_name>/<baselinemodel>/<XAI>.jpg\n",
      "Starting comprehensive heatmap generation...\n",
      "Tasks: ['DME']\n",
      "Models: ['ResNet-50']\n",
      "Samples per task: 1\n",
      "\n",
      "=== Processing Task: DME ===\n",
      "Loaded 1 images for DME\n",
      "\n",
      "--- Processing Model: ResNet-50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e10fba9ec040fb937f1bd853b0f320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-50 and are newly initialized because the shapes did not match:\n",
      "- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.1.weight: found shape torch.Size([1000, 2048]) in the checkpoint and torch.Size([2, 2048]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume checkpoint /blue/ruogu.fang/tienyuchang/RETFound_MAE/output_dir/DME_binary_all_split-IRB2024_v5-all-microsoft/resnet-50-OCT-bs16ep50lr5e-4optadamw-defaulteval-trsub0--/checkpoint-best.pth for ResNet-50 on DME\n",
      "‚úì GradCAM initialized for ResNet-50 (patch_size: 7)\n",
      "‚úì ScoreCAM initialized for ResNet-50 (patch_size: 7)\n",
      "Masks are loaded.\n",
      "‚úì RISE initialized for ResNet-50 (gpu_batch: 1)\n",
      "‚úì CRP_LXT initialized for ResNet-50 (gpu_batch: 1)\n",
      "‚ö† Attention skipped for ResNet-50 (not a transformer model)\n",
      "Processing image 1/1 (Label: 0, File: 1.2.840.114158.49546168971479733704162688413692655539_latR_13)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m N_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#heatmap_results_v2 = generate_comprehensive_heatmaps_v2(num_samples=1,task_list=Task_list,model_list=Model_list)  # Start with 3 samples for testing\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m heatmap_results_v2 \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_comprehensive_heatmaps_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtask_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDME\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mResNet-50\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Start with 3 samples for testing\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 43\u001b[0m, in \u001b[0;36mgenerate_comprehensive_heatmaps_v2\u001b[0;34m(num_samples, task_list, model_list)\u001b[0m\n\u001b[1;32m     41\u001b[0m image_tensor \u001b[38;5;241m=\u001b[39m preprocess_image(image, processor, input_size)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Generate all heatmaps for this image\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m heatmaps \u001b[38;5;241m=\u001b[39m \u001b[43mxai_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_all_heatmaps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m results[task][model_name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheatmaps\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(heatmaps)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xai_name, heatmap \u001b[38;5;129;01min\u001b[39;00m heatmaps\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[11], line 198\u001b[0m, in \u001b[0;36mXAIGenerator.generate_all_heatmaps\u001b[0;34m(self, image_tensor, target_class)\u001b[0m\n\u001b[1;32m    195\u001b[0m heatmaps \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# GradCAM\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m gradcam_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_gradcam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_class\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gradcam_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     heatmaps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGradCAM\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m gradcam_map\n",
      "Cell \u001b[0;32mIn[11], line 137\u001b[0m, in \u001b[0;36mXAIGenerator.generate_gradcam\u001b[0;34m(self, image_tensor, target_class)\u001b[0m\n\u001b[1;32m    134\u001b[0m         target_class \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    136\u001b[0m targets \u001b[38;5;241m=\u001b[39m [ClassifierOutputTarget(target_class)]\n\u001b[0;32m--> 137\u001b[0m heatmap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradcam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m heatmap[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(heatmap) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/blue/guoj1/tienyuchang/.conda/envs/octxai/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/guoj1/tienyuchang/.conda/envs/octxai/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/blue/ruogu.fang/tienyuchang/RETFound_MAE/baselines/GradCAM_v2.py:249\u001b[0m, in \u001b[0;36mPytorchCAM.forward\u001b[0;34m(self, pixel_values, targets_for_gradcam)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pixel_values, targets_for_gradcam: List[Callable]):\n\u001b[0;32m--> 249\u001b[0m     cam_bs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_cam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_for_gradcam\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# back to original image size\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     cam_bs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(cam_bs\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/blue/ruogu.fang/tienyuchang/RETFound_MAE/baselines/GradCAM_v2.py:237\u001b[0m, in \u001b[0;36mPytorchCAM.compute_cam\u001b[0;34m(self, pixel_values, targets_for_gradcam)\u001b[0m\n\u001b[1;32m    235\u001b[0m     targets_expanded \u001b[38;5;241m=\u001b[39m targets_for_gradcam\n\u001b[1;32m    236\u001b[0m     repeated_tensor \u001b[38;5;241m=\u001b[39m pixel_values\n\u001b[0;32m--> 237\u001b[0m batch_results \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeated_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargets_expanded\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# shape: (B', H', W')\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# Normalize per image\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_cam:\n",
      "File \u001b[0;32m/blue/guoj1/tienyuchang/.conda/envs/octxai/lib/python3.9/site-packages/pytorch_grad_cam/base_cam.py:209\u001b[0m, in \u001b[0;36mBaseCAM.__call__\u001b[0;34m(self, input_tensor, targets, aug_smooth, eigen_smooth)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aug_smooth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_augmentation_smoothing(input_tensor, targets, eigen_smooth)\n\u001b[0;32m--> 209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/guoj1/tienyuchang/.conda/envs/octxai/lib/python3.9/site-packages/pytorch_grad_cam/base_cam.py:111\u001b[0m, in \u001b[0;36mBaseCAM.forward\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m    109\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([target(output) \u001b[38;5;28;01mfor\u001b[39;00m target, output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(targets, outputs)])\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetach:\n\u001b[0;32m--> 111\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# keep the computational graph, create_graph = True is needed for hvp\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(loss, input_tensor, retain_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, create_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/blue/guoj1/tienyuchang/.conda/envs/octxai/lib/python3.9/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/guoj1/tienyuchang/.conda/envs/octxai/lib/python3.9/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/guoj1/tienyuchang/.conda/envs/octxai/lib/python3.9/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# Test the new function with improved directory structure\n",
    "print(\"=== Testing New Heatmap Generation Function ===\")\n",
    "print(\"This will save heatmaps in the structure: ./heatmap_results/<task_name>/<label_idx>/<image_name>/<baselinemodel>/<XAI>.jpg\")\n",
    "\n",
    "# Test with a small number of samples first\n",
    "#Model_list = ['ResNet-50', 'timm_efficientnet-b4', 'vit-base-patch16-224', 'RETFound_mae']\n",
    "N_samples = 5\n",
    "#heatmap_results_v2 = generate_comprehensive_heatmaps_v2(num_samples=1,task_list=Task_list,model_list=Model_list)  # Start with 3 samples for testing\n",
    "heatmap_results_v2 = generate_comprehensive_heatmaps_v2(num_samples=1,task_list=['DME'],model_list=['ResNet-50'])  # Start with 3 samples for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da2512c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating visualization for DME...\n",
      "Heatmap grid saved to heatmap_results/DME/heatmap_grid.png\n",
      "Heatmap grid saved to heatmap_results/DME/heatmap_grid.png\n",
      "Heatmap grid saved to heatmap_results/DME/heatmap_grid.png\n",
      "Heatmap grid saved to heatmap_results/DME/heatmap_grid.png\n",
      "Heatmap grid saved to heatmap_results/DME/heatmap_grid.png\n"
     ]
    }
   ],
   "source": [
    "# Create individual task-specific visualizations\n",
    "def create_task_specific_grids(results_dict, output_dir, num_samples):\n",
    "    \"\"\"Create separate visualizations for each task\"\"\"\n",
    "    for task in results_dict.keys():\n",
    "        print(f\"Creating visualization for {task}...\")\n",
    "        # Create task-specific results dict\n",
    "        task_results = {task: results_dict[task]}\n",
    "        # Create visualization\n",
    "        task_grid_path = output_dir / task / \"heatmap_grid.png\"\n",
    "        create_heatmap_grid(task_results, task, num_samples=num_samples, save_path=task_grid_path)\n",
    "\n",
    "# Generate task-specific grids\n",
    "output_dir = Path(\"./heatmap_results\") \n",
    "create_task_specific_grids(heatmap_results_v2, output_dir, N_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37a37cf-fb8c-4a2c-90fb-1fa83f32edf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "octxai",
   "language": "python",
   "name": "octxai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
