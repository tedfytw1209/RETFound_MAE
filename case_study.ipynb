{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b38c954-c9b3-41f2-808b-c16d81e7a700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set environment variable to avoid symbolic tracing issues\n",
    "import os\n",
    "os.environ['TIMM_FUSED_ATTN'] = '0'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "from pytorch_grad_cam import run_dff_on_image, GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from typing import List, Callable, Optional\n",
    "\n",
    "# Import XAI methods\n",
    "from baselines.GradCAM_v2 import PytorchCAM\n",
    "from baselines.RISE import RISEBatch\n",
    "from baselines.Attention import Attention_Map\n",
    "from baselines.ViT_explanation_generator import LRP, Baselines\n",
    "from pytorch_grad_cam import GradCAM, ScoreCAM\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "088415c1-dba0-4047-aee5-53bc1fabac72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    ViTImageProcessor, ViTForImageClassification,\n",
    "    AutoImageProcessor, EfficientNetForImageClassification,\n",
    "    ResNetForImageClassification, AutoModel\n",
    ")\n",
    "import models_vit as models\n",
    "from util.datasets import TransformWrapper\n",
    "import timm\n",
    "\n",
    "#get model\n",
    "def get_model(task,model,input_size,nb_classes):\n",
    "    if 'ADCon' in task:\n",
    "        id2label = {0: \"control\", 1: \"ad\"}\n",
    "        label2id = {v: k for k, v in id2label.items()}\n",
    "    else:\n",
    "        id2label = {i: f\"class_{i}\" for i in range(nb_classes)}\n",
    "        label2id = {v: k for k, v in id2label.items()}\n",
    "    processor = None\n",
    "    if 'RETFound_mae' in model:\n",
    "        model = models.__dict__['RETFound_mae'](\n",
    "        img_size=input_size,\n",
    "        num_classes=nb_classes,\n",
    "        drop_path_rate=0.2,\n",
    "        global_pool=True,\n",
    "    )\n",
    "    elif 'vit-base-patch16-224' in model:\n",
    "        # ViT-base-patch16-224 preprocessor\n",
    "        model_ = 'google/vit-base-patch16-224'\n",
    "        processor = TransformWrapper(ViTImageProcessor.from_pretrained(model_))\n",
    "        model = ViTForImageClassification.from_pretrained(\n",
    "            model_,\n",
    "            image_size=input_size, #Not in tianhao code, default 224\n",
    "            num_labels=nb_classes,\n",
    "            hidden_dropout_prob=0.0, #Not in tianhao code, default 0.0\n",
    "            attention_probs_dropout_prob=0.0, #Not in tianhao code, default 0.0\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "            ignore_mismatched_sizes=True,\n",
    "            attn_implementation=\"eager\",      # ‚Üê key line\n",
    "        )\n",
    "        model.config.return_dict = True\n",
    "        model.config.output_attentions = True\n",
    "    elif 'timm_efficientnet-b4' in model:\n",
    "        model = timm.create_model('efficientnet_b4', pretrained=True, num_classes=nb_classes)\n",
    "        processor  = transforms.Compose([\n",
    "            transforms.Resize((380,380)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "        ])\n",
    "    elif 'ResNet-50' in model:\n",
    "        model_name = 'microsoft/resnet-50'\n",
    "        processor = TransformWrapper(AutoImageProcessor.from_pretrained(model_name))\n",
    "        model = ResNetForImageClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=nb_classes,\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        model.config.return_dict = True\n",
    "        model.config.output_attentions = True\n",
    "\n",
    "    return model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b94f0a85-078e-4a03-a4f7-57bf8778c49c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# task and dataset\n",
    "#Task_list = ['ADCon','DME']\n",
    "Task_list = ['DME']\n",
    "dataset_fname = 'sampled_labels01.csv'\n",
    "dataset_dir = '/blue/ruogu.fang/tienyuchang/OCT_EDA'\n",
    "img_p_fmt = \"label_%d/%s\" #label index and oct_img name\n",
    "\n",
    "# model\n",
    "Model_root = \"/blue/ruogu.fang/tienyuchang/RETFound_MAE/output_dir\"\n",
    "Model_fname = \"checkpoint-best.pth\"\n",
    "Model_list = ['ResNet-50', 'timm_efficientnet-b4', 'vit-base-patch16-224', 'RETFound_mae']\n",
    "ADCon_finetuned = [\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"ad_control_detect_data-IRB2024v5_ADCON_DL_data-all-RETFound_mae-OCT-defaulteval---bal_sampler-/\"\n",
    "]\n",
    "DME_finetuned = [\n",
    "    \"DME_binary_all_split-IRB2024_v5-all-microsoft/resnet-50-OCT-bs16ep50lr5e-4optadamw-defaulteval-trsub0--/\",\n",
    "    \"DME_binary_all_split-IRB2024_v5-all-timm_efficientnet-b4-OCT-bs16ep50lr5e-4optadamw-defaulteval-trsub0--/\",\n",
    "    \"DME_binary_all_split-IRB2024_v5-all-google/vit-base-patch16-224-in21k-OCT-bs16ep50lr5e-4optadamw-defaulteval-trsub0--/\",\n",
    "    \"DME_binary_all_split-IRB2024_v5-all-RETFound_mae_natureOCT-OCT-bs16ep50lr5e-4optadamw-roc_auceval-trsub0--/\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3009084",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "<PIL.Image.Image image mode=RGB size=512x496 at 0x14CB5E45F310>\n",
      "[0, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Data loading and preprocessing functions\n",
    "def load_sample_data(task, num_sample=-1):\n",
    "    \"\"\"Load sample images for a given task\"\"\"\n",
    "    df = pd.read_csv(os.path.join(dataset_dir, \"%s_sampled\"%task, dataset_fname))\n",
    "    task_df = df[df['label'].isin([0, 1])]  # Adjust based on actual DME labels\n",
    "    # Sample random images\n",
    "    if num_sample > 0:\n",
    "        task_df = task_df.sample(n=num_sample, random_state=42).reset_index(drop=True)\n",
    "    else:\n",
    "        task_df = task_df.reset_index(drop=True)\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    filenames = []\n",
    "    \n",
    "    for _, row in task_df.iterrows():\n",
    "        # Extract just the filename from oct_img\n",
    "        filename = os.path.basename(row['OCT']) if isinstance(row['OCT'], str) else row['OCT']\n",
    "        img_path = os.path.join(dataset_dir, \"%s_sampled\"%task, img_p_fmt % (row['label'], filename))\n",
    "        if os.path.exists(img_path):\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                images.append(img)\n",
    "                labels.append(row['label'])\n",
    "                # Store filename without extension for directory naming\n",
    "                image_name = os.path.splitext(filename)[0]\n",
    "                filenames.append(image_name)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_path}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return images, labels, filenames\n",
    "\n",
    "def preprocess_image(image, processor=None, input_size=224, device=None, dtype=torch.float32):\n",
    "    \"\"\"\n",
    "    Â∞Ü PIL.Image -> [1,3,H,W] tensorÔºàfloat32ÔºâÔºåËá™Âä®ÈÄÇÈÖçÔºö\n",
    "      - timm/torchvision transformsÔºöprocessor(image)\n",
    "      - HuggingFace ImageProcessorÔºöprocessor(image, return_tensors=\"pt\") ÊàñËøîÂõû dict\n",
    "      - ÊúÄÂêéÂõûÈÄÄÂà∞Ê†áÂáÜ ImageNet È¢ÑÂ§ÑÁêÜ\n",
    "    \"\"\"\n",
    "    assert isinstance(image, Image.Image), f\"expect PIL.Image, got {type(image)}\"\n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if processor is not None:\n",
    "        # A) ÂÖàÂ∞ùËØï‚ÄúÁõ¥Êé•ÂèØË∞ÉÁî®‚ÄùÂΩ¢ÂºèÔºàÂ§öÊï∞ timm/torchvision transformÔºâ\n",
    "        try:\n",
    "            out = processor(image)\n",
    "            if isinstance(out, torch.Tensor):\n",
    "                x = out\n",
    "                if x.ndim == 3:  # [C,H,W] -> [1,C,H,W]\n",
    "                    x = x.unsqueeze(0)\n",
    "                return x.to(device=device, dtype=dtype)\n",
    "            if isinstance(out, dict) and \"pixel_values\" in out:\n",
    "                x = out[\"pixel_values\"]\n",
    "                if isinstance(x, np.ndarray):\n",
    "                    x = torch.from_numpy(x)\n",
    "                if x.ndim == 3:\n",
    "                    x = x.unsqueeze(0)\n",
    "                return x.to(device=device, dtype=dtype)\n",
    "        except TypeError:\n",
    "            pass\n",
    "\n",
    "        # B) ÂÜçÂ∞ùËØï HuggingFace È£éÊ†ºÔºà‰∏ç‰ΩøÁî® images= ÂÖ≥ÈîÆÂ≠óÔºâ\n",
    "        try:\n",
    "            out = processor(image, return_tensors=\"pt\")\n",
    "            if isinstance(out, dict) and \"pixel_values\" in out:\n",
    "                x = out[\"pixel_values\"]  # [1,3,H,W]\n",
    "                return x.to(device=device, dtype=dtype)\n",
    "            if isinstance(out, torch.Tensor):\n",
    "                x = out\n",
    "                if x.ndim == 3:\n",
    "                    x = x.unsqueeze(0)\n",
    "                return x.to(device=device, dtype=dtype)\n",
    "        except TypeError:\n",
    "            pass\n",
    "\n",
    "        # C) Êüê‰∫õÂÆûÁé∞‰ªÖÊé•ÂèóÂàóË°®\n",
    "        for attempt in (lambda: processor([image], return_tensors=\"pt\"),\n",
    "                        lambda: processor([image])):\n",
    "            try:\n",
    "                out = attempt()\n",
    "                if isinstance(out, dict) and \"pixel_values\" in out:\n",
    "                    x = out[\"pixel_values\"]\n",
    "                    if isinstance(x, np.ndarray):\n",
    "                        x = torch.from_numpy(x)\n",
    "                    return x.to(device=device, dtype=dtype)\n",
    "                if isinstance(out, torch.Tensor):\n",
    "                    x = out\n",
    "                    if x.ndim == 3:\n",
    "                        x = x.unsqueeze(0)\n",
    "                    return x.to(device=device, dtype=dtype)\n",
    "            except TypeError:\n",
    "                pass\n",
    "\n",
    "    # D) ÂõûÈÄÄÔºöÊ†áÂáÜ ImageNet È¢ÑÂ§ÑÁêÜ\n",
    "    fallback = transforms.Compose([\n",
    "        transforms.Resize((input_size, input_size)),\n",
    "        transforms.ToTensor(),  # [0,1]\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    x = fallback(image)            # [3,H,W]\n",
    "    x = x.unsqueeze(0)             # [1,3,H,W]\n",
    "    return x.to(device=device, dtype=dtype)\n",
    "\n",
    "#test dataset\n",
    "dme_imgs, dme_labels, dme_img_names = load_sample_data('DME',5)\n",
    "print(len(dme_imgs))\n",
    "print(dme_imgs[0])\n",
    "print(dme_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcdfdc79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel_list = []\\nfor model_name in Model_list:\\n    model, processor = load_trained_model('DME', model_name, 224)\\n    print(type(model))\\n    model_list.append(model)\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load trained models function\n",
    "def load_trained_model(task, model_name, input_size=224, nb_classes=2):\n",
    "    \"\"\"Load a trained model for a specific task\"\"\"\n",
    "    model, processor = get_model(task, model_name, input_size, nb_classes)\n",
    "    \n",
    "    # Load model weights based on task and model\n",
    "    if task == 'ADCon':\n",
    "        model_paths = ADCon_finetuned\n",
    "    elif task == 'DME':\n",
    "        model_paths = DME_finetuned\n",
    "    else:\n",
    "        print(f\"Unknown task: {task}\")\n",
    "        model.eval()\n",
    "        return model, processor\n",
    "    \n",
    "    model_idx = Model_list.index(model_name)\n",
    "    model_dir = model_paths[model_idx]\n",
    "    model_path = os.path.join(Model_root, model_dir, Model_fname)\n",
    "    \n",
    "    # Load finetuned model if specified (following main_XAI_evaluation.py pattern)\n",
    "    if model_path and model_path != '':\n",
    "        if os.path.exists(model_path):\n",
    "            try:\n",
    "                # Load checkpoint\n",
    "                if model_path.startswith('https'):\n",
    "                    checkpoint = torch.hub.load_state_dict_from_url(\n",
    "                        model_path, map_location='cpu', check_hash=True)\n",
    "                else:\n",
    "                    checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)\n",
    "                \n",
    "                # Extract model state dict\n",
    "                if 'model' in checkpoint:\n",
    "                    checkpoint_model = checkpoint['model']\n",
    "                else:\n",
    "                    checkpoint_model = checkpoint\n",
    "                \n",
    "                # Load with strict=False to handle potential mismatches\n",
    "                model.load_state_dict(checkpoint_model, strict=False)\n",
    "                print(f\"Resume checkpoint {model_path} for {model_name} on {task}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading model {model_name} for {task}: {e}\")\n",
    "                print(\"Using pretrained weights instead\")\n",
    "        else:\n",
    "            print(f\"Model path not found: {model_path}\")\n",
    "            print(f\"Using pretrained weights for {model_name} on {task}\")\n",
    "    else:\n",
    "        print(f\"No checkpoint specified for {model_name} on {task}, using pretrained weights\")\n",
    "    \n",
    "    model.eval()\n",
    "    return model, processor\n",
    "\n",
    "#test\n",
    "'''\n",
    "model_list = []\n",
    "for model_name in Model_list:\n",
    "    model, processor = load_trained_model('DME', model_name, 224)\n",
    "    print(type(model))\n",
    "    model_list.append(model)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d00dd935",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#test\\nfor model_name in Model_list:\\n    model, processor = load_trained_model('DME', model_name, 224)\\n    XAIGenerator(model, model_name)\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XAI Methods Implementation\n",
    "class XAIGenerator:\n",
    "    def __init__(self, model, model_name, input_size=224):\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.input_size = input_size\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # Initialize XAI methods\n",
    "        self.init_xai_methods()\n",
    "    \n",
    "    def get_model_specific_config(self):\n",
    "        \"\"\"Get model-specific configuration for XAI methods\"\"\"\n",
    "        config = {\n",
    "            'patch_size': 14,\n",
    "            'gpu_batch': 1,\n",
    "            'attention_layers': 12\n",
    "        }\n",
    "        \n",
    "        # Model-specific configurations\n",
    "        if 'resnet' in self.model_name.lower():\n",
    "            config.update({\n",
    "                'patch_size': 7,  # ResNet has different spatial resolution\n",
    "                'gpu_batch': 1,  # ResNet can handle larger batches\n",
    "            })\n",
    "        elif 'efficientnet' in self.model_name.lower():\n",
    "            config.update({\n",
    "                'patch_size': 7,  # EfficientNet spatial resolution\n",
    "                'gpu_batch': 1,\n",
    "            })\n",
    "        elif 'vit' in self.model_name.lower():\n",
    "            config.update({\n",
    "                'patch_size': 16,  # ViT patch size\n",
    "                'gpu_batch': 1,\n",
    "                'attention_layers': 12,  # Standard ViT-Base layers\n",
    "            })\n",
    "        elif 'retfound' in self.model_name.lower():\n",
    "            config.update({\n",
    "                'patch_size': 16,  # RETFound uses ViT architecture\n",
    "                'gpu_batch': 1,\n",
    "                'attention_layers': 12,\n",
    "            })\n",
    "        \n",
    "        return config\n",
    "    \n",
    "    def init_xai_methods(self):\n",
    "        \"\"\"Initialize all XAI methods with model-specific configurations\"\"\"\n",
    "        config = self.get_model_specific_config()\n",
    "        # GradCAM with model-specific config\n",
    "        self.gradcam = PytorchCAM(\n",
    "            self.model, \n",
    "            self.model_name, \n",
    "            self.input_size, \n",
    "            patch_size=config['patch_size'],\n",
    "            method=GradCAM\n",
    "        )\n",
    "        print(f\"‚úì GradCAM initialized for {self.model_name} (patch_size: {config['patch_size']})\")\n",
    "        \n",
    "        # ScoreCAM with model-specific config\n",
    "        self.scorecam = PytorchCAM(\n",
    "            self.model, \n",
    "            self.model_name, \n",
    "            self.input_size, \n",
    "            patch_size=config['patch_size'],\n",
    "            method=ScoreCAM\n",
    "        )\n",
    "        print(f\"‚úì ScoreCAM initialized for {self.model_name} (patch_size: {config['patch_size']})\")\n",
    "        \n",
    "        # RISE with model-specific batch size\n",
    "        # Reduce batch for memory-heavy models\n",
    "        rise_batch = config['gpu_batch']\n",
    "        if 'retfound' in self.model_name.lower():\n",
    "            rise_batch = min(rise_batch, 16)\n",
    "        self.rise = RISEBatch(\n",
    "            self.model, \n",
    "            input_size=(self.input_size, self.input_size), \n",
    "            gpu_batch=rise_batch,\n",
    "            N=50\n",
    "        )\n",
    "        print(f\"‚úì RISE initialized for {self.model_name} (gpu_batch: {rise_batch})\")\n",
    "        \n",
    "        try:\n",
    "            # Attention Maps (only for ViT-based models)\n",
    "            if 'vit' in self.model_name.lower() or 'retfound' in self.model_name.lower():\n",
    "                self.attention = Attention_Map(\n",
    "                    self.model, \n",
    "                    self.model_name, \n",
    "                    self.input_size, \n",
    "                    N=config['attention_layers'],\n",
    "                    use_rollout=True,\n",
    "                    print_layers=False  # Disable layer printing to avoid issues\n",
    "                )\n",
    "                print(f\"‚úì Attention initialized for {self.model_name} (layers: {config['attention_layers']})\")\n",
    "            else:\n",
    "                self.attention = None\n",
    "                print(f\"‚ö† Attention skipped for {self.model_name} (not a transformer model)\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Failed to initialize Attention for {self.model_name}: {e}\")\n",
    "            print(f\"  Note: This may be due to symbolic tracing issues with the model architecture\")\n",
    "            \n",
    "            # Try alternative attention extraction for RETFound\n",
    "            if 'retfound' in self.model_name.lower():\n",
    "                try:\n",
    "                    print(f\"  üîÑ Trying alternative attention extraction for RETFound...\")\n",
    "                    self.attention = self._create_simple_attention_extractor()\n",
    "                    print(f\"‚úì Alternative attention method initialized for {self.model_name}\")\n",
    "                except Exception as e2:\n",
    "                    print(f\"‚úó Alternative attention method also failed: {e2}\")\n",
    "                    self.attention = None\n",
    "            else:\n",
    "                self.attention = None\n",
    "        \n",
    "        # LRP (requires special model implementation)\n",
    "        self.lrp = None  # Will implement if model supports it\n",
    "    \n",
    "    def generate_gradcam(self, image_tensor, target_class=None):\n",
    "        \"\"\"Generate GradCAM heatmap\"\"\"\n",
    "        if self.gradcam is None:\n",
    "            return None\n",
    "        image_tensor = image_tensor.to(self.device)\n",
    "        if target_class is None:\n",
    "            # Get predicted class\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(image_tensor)\n",
    "                target_class = outputs.argmax(dim=1).item()\n",
    "\n",
    "        targets = [ClassifierOutputTarget(target_class)]\n",
    "        heatmap = self.gradcam(image_tensor, targets)\n",
    "        return heatmap[0] if len(heatmap) > 0 else None\n",
    "    \n",
    "    def generate_scorecam(self, image_tensor, target_class=None):\n",
    "        \"\"\"Generate ScoreCAM heatmap\"\"\"\n",
    "        if self.scorecam is None:\n",
    "            return None\n",
    "        image_tensor = image_tensor.to(self.device)\n",
    "        if target_class is None:\n",
    "            # Get predicted class\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(image_tensor)\n",
    "                target_class = outputs.argmax(dim=1).item()\n",
    "\n",
    "        targets = [ClassifierOutputTarget(target_class)]\n",
    "        heatmap = self.scorecam(image_tensor, targets)\n",
    "        return heatmap[0] if len(heatmap) > 0 else None\n",
    "    \n",
    "    def generate_rise(self, image_tensor, target_class=None):\n",
    "        \"\"\"Generate RISE heatmap\"\"\"\n",
    "        if self.rise is None:\n",
    "            return None\n",
    "        image_tensor = image_tensor.to(self.device)\n",
    "\n",
    "        if target_class is None:\n",
    "            # Get predicted class\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(image_tensor)\n",
    "                target_class = outputs.argmax(dim=1).item()\n",
    "\n",
    "        heatmaps = self.rise(image_tensor)\n",
    "        return heatmaps[0, target_class] if heatmaps is not None else None\n",
    "    \n",
    "    def generate_attention(self, image_tensor):\n",
    "        \"\"\"Generate Attention heatmap\"\"\"\n",
    "        if self.attention is None:\n",
    "            return None\n",
    "        image_tensor = image_tensor.to(self.device)\n",
    "        attention_map = self.attention(image_tensor)\n",
    "        return attention_map[0] if attention_map is not None else None\n",
    "    \n",
    "    def generate_all_heatmaps(self, image_tensor, target_class=None):\n",
    "        \"\"\"Generate all available heatmaps for an image\"\"\"\n",
    "        heatmaps = {}\n",
    "        \n",
    "        # GradCAM\n",
    "        gradcam_map = self.generate_gradcam(image_tensor, target_class)\n",
    "        if gradcam_map is not None:\n",
    "            heatmaps['GradCAM'] = gradcam_map\n",
    "        \n",
    "        # ScoreCAM\n",
    "        scorecam_map = self.generate_scorecam(image_tensor, target_class)\n",
    "        if scorecam_map is not None:\n",
    "            heatmaps['ScoreCAM'] = scorecam_map\n",
    "        \n",
    "        # RISE\n",
    "        '''\n",
    "        rise_map = self.generate_rise(image_tensor, target_class)\n",
    "        if rise_map is not None:\n",
    "            heatmaps['RISE'] = rise_map\n",
    "        '''\n",
    "        \n",
    "        # Attention\n",
    "        attention_map = self.generate_attention(image_tensor)\n",
    "        if attention_map is not None:\n",
    "            heatmaps['Attention'] = attention_map\n",
    "        \n",
    "        return heatmaps\n",
    "    \n",
    "    def _create_simple_attention_extractor(self):\n",
    "        \"\"\"Create a simple attention extractor that doesn't use symbolic tracing\"\"\"\n",
    "        class SimpleAttentionExtractor:\n",
    "            def __init__(self, model, model_name, input_size):\n",
    "                self.model = model\n",
    "                self.model_name = model_name\n",
    "                self.input_size = input_size\n",
    "                self.attention_maps = []\n",
    "                \n",
    "                # Register hooks to capture attention weights\n",
    "                self.hooks = []\n",
    "                if hasattr(model, 'blocks'):\n",
    "                    for i, block in enumerate(model.blocks):\n",
    "                        if hasattr(block, 'attn'):\n",
    "                            hook = block.attn.register_forward_hook(self._attention_hook)\n",
    "                            self.hooks.append(hook)\n",
    "            \n",
    "            def _attention_hook(self, module, input, output):\n",
    "                \"\"\"Hook to capture attention weights\"\"\"\n",
    "                # Try to get attention weights from different possible attributes\n",
    "                if hasattr(module, 'attention_probs'):\n",
    "                    self.attention_maps.append(module.attention_probs.detach())\n",
    "                elif hasattr(module, 'attn_weights'):\n",
    "                    self.attention_maps.append(module.attn_weights.detach())\n",
    "                elif len(output) > 1 and isinstance(output[1], torch.Tensor):\n",
    "                    # Some models return attention as second output\n",
    "                    self.attention_maps.append(output[1].detach())\n",
    "            \n",
    "            def __call__(self, x):\n",
    "                \"\"\"Extract attention maps\"\"\"\n",
    "                self.attention_maps = []\n",
    "                \n",
    "                # Forward pass to trigger hooks\n",
    "                with torch.no_grad():\n",
    "                    _ = self.model(x)\n",
    "                \n",
    "                if len(self.attention_maps) > 0:\n",
    "                    try:\n",
    "                        # Use the last attention layer\n",
    "                        last_attention = self.attention_maps[-1]  # Shape: (B, num_heads, num_tokens, num_tokens)\n",
    "                        \n",
    "                        # Average over heads and extract CLS attention to patches\n",
    "                        if len(last_attention.shape) == 4:  # (B, num_heads, num_tokens, num_tokens)\n",
    "                            cls_attention = last_attention.mean(dim=1)[:, 0, 1:]  # (B, num_patches)\n",
    "                        else:\n",
    "                            # Fallback for different attention shapes\n",
    "                            cls_attention = last_attention[:, 0, 1:] if last_attention.shape[1] > 1 else last_attention[:, 0]\n",
    "                        \n",
    "                        # Reshape to spatial dimensions\n",
    "                        B = cls_attention.shape[0]\n",
    "                        num_patches = cls_attention.shape[1]\n",
    "                        patch_size = int(num_patches ** 0.5)\n",
    "                        \n",
    "                        if patch_size * patch_size == num_patches:\n",
    "                            attention_maps = cls_attention.reshape(B, patch_size, patch_size)\n",
    "                            \n",
    "                            # Resize to input size\n",
    "                            attention_maps_resized = torch.nn.functional.interpolate(\n",
    "                                attention_maps.unsqueeze(1), \n",
    "                                size=(self.input_size, self.input_size), \n",
    "                                mode='bilinear', \n",
    "                                align_corners=False\n",
    "                            ).squeeze(1)\n",
    "                            \n",
    "                            return attention_maps_resized.cpu().numpy()\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing attention maps: {e}\")\n",
    "                \n",
    "                # Fallback: return zeros\n",
    "                return np.zeros((x.shape[0], self.input_size, self.input_size))\n",
    "            \n",
    "            def cleanup(self):\n",
    "                \"\"\"Remove hooks\"\"\"\n",
    "                for hook in self.hooks:\n",
    "                    hook.remove()\n",
    "        \n",
    "        return SimpleAttentionExtractor(self.model, self.model_name, self.input_size)\n",
    "'''\n",
    "#test\n",
    "for model_name in Model_list:\n",
    "    model, processor = load_trained_model('DME', model_name, 224)\n",
    "    XAIGenerator(model, model_name)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "063fabda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualization functions\n",
    "def normalize_heatmap(heatmap):\n",
    "    \"\"\"Normalize heatmap to 0-1 range\"\"\"\n",
    "    if heatmap is None:\n",
    "        return None\n",
    "    \n",
    "    heatmap = np.array(heatmap)\n",
    "    if heatmap.max() == heatmap.min():\n",
    "        return np.zeros_like(heatmap)\n",
    "    \n",
    "    return (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
    "\n",
    "def overlay_heatmap_on_image(image, heatmap, alpha=0.4, colormap='jet'):\n",
    "    \"\"\"Overlay heatmap on original image\"\"\"\n",
    "    if heatmap is None:\n",
    "        return np.array(image)\n",
    "    \n",
    "    # Normalize heatmap\n",
    "    heatmap_norm = normalize_heatmap(heatmap)\n",
    "    \n",
    "    # Resize heatmap to match image size\n",
    "    if isinstance(image, Image.Image):\n",
    "        image_array = np.array(image)\n",
    "        image_size = image.size\n",
    "    else:\n",
    "        image_array = image\n",
    "        image_size = (image.shape[1], image.shape[0])\n",
    "    \n",
    "    heatmap_resized = cv2.resize(heatmap_norm, image_size)\n",
    "    \n",
    "    # Apply colormap\n",
    "    cmap = plt.get_cmap(colormap)\n",
    "    heatmap_colored = cmap(heatmap_resized)[:, :, :3]  # Remove alpha channel\n",
    "    \n",
    "    # Normalize image\n",
    "    image_norm = image_array.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Overlay\n",
    "    overlay = alpha * heatmap_colored + (1 - alpha) * image_norm\n",
    "    overlay = np.clip(overlay, 0, 1)\n",
    "    \n",
    "    return (overlay * 255).astype(np.uint8)\n",
    "\n",
    "def create_heatmap_grid(results_dict, task, num_samples, save_path=None):\n",
    "    \"\"\"Create a comprehensive grid visualization of all heatmaps\"\"\"\n",
    "    models = Model_list\n",
    "    #xai_methods = ['GradCAM', 'ScoreCAM', 'RISE', 'Attention']\n",
    "    xai_methods = ['GradCAM', 'ScoreCAM', 'Attention']\n",
    "    \n",
    "    # Calculate grid dimensions\n",
    "    n_models = len(models)\n",
    "    n_methods = len(xai_methods)\n",
    "    \n",
    "    for sample_idx in range(num_samples):\n",
    "        # Create figure\n",
    "        fig_width = max(20, n_methods * 4)\n",
    "        fig_height = max(15, n_models * 3)\n",
    "        fig, axes = plt.subplots(\n",
    "            n_models, \n",
    "            n_methods + 1,  # +1 for original image\n",
    "            figsize=(fig_width, fig_height)\n",
    "        )\n",
    "        if n_models == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        elif len(axes.shape) == 1:\n",
    "            axes = axes.reshape(-1, 1)\n",
    "        for model_idx, model in enumerate(models):\n",
    "            if model not in results_dict[task]:\n",
    "                continue\n",
    "            model_results = results_dict[task][model]\n",
    "            images = model_results['images']\n",
    "            labels = model_results['labels']\n",
    "            heatmaps = model_results['heatmaps']\n",
    "            \n",
    "            image = images[sample_idx]\n",
    "            label = labels[sample_idx]\n",
    "            sample_heatmaps = heatmaps[sample_idx]\n",
    "\n",
    "            # Original image\n",
    "            axes[model_idx, 0].imshow(image)\n",
    "            axes[model_idx, 0].set_title(f'{task}-{model}\\nSample {sample_idx+1} (Label: {label})')\n",
    "            axes[model_idx, 0].axis('off')\n",
    "            # Heatmaps\n",
    "            for method_idx, method in enumerate(xai_methods):\n",
    "                col_idx = method_idx + 1\n",
    "                \n",
    "                if method in sample_heatmaps and sample_heatmaps[method] is not None:\n",
    "                    overlay = overlay_heatmap_on_image(image, sample_heatmaps[method])\n",
    "                    axes[model_idx, col_idx].imshow(overlay)\n",
    "                    axes[model_idx, col_idx].set_title(f'{method}')\n",
    "                else:\n",
    "                    axes[model_idx, col_idx].text(0.5, 0.5, 'N/A', \n",
    "                                                ha='center', va='center', \n",
    "                                                transform=axes[model_idx, col_idx].transAxes)\n",
    "                    axes[model_idx, col_idx].set_title(f'{method}')\n",
    "                axes[model_idx, col_idx].axis('off')\n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            sample_path = save_path.parent / f\"{save_path.stem}_{sample_idx}{save_path.suffix}\"\n",
    "            plt.savefig(sample_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Heatmap grid saved to {save_path}\")\n",
    "        plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "596566fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Updated function with new directory structure for heatmap saving\n",
    "def generate_comprehensive_heatmaps_v2(num_samples=3,task_list=Task_list,model_list=Model_list):\n",
    "    \"\"\"Generate heatmaps for all task-model combinations with new directory structure\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    input_size = 224\n",
    "    \n",
    "    print(\"Starting comprehensive heatmap generation...\")\n",
    "    print(f\"Tasks: {task_list}\")\n",
    "    print(f\"Models: {model_list}\")\n",
    "    print(f\"Samples per task: {num_samples}\")\n",
    "    \n",
    "    for task in task_list:\n",
    "        print(f\"\\n=== Processing Task: {task} ===\")\n",
    "        results[task] = {}\n",
    "        \n",
    "        # Load sample data for this task (now returns filenames too)\n",
    "        try:\n",
    "            images, labels, filenames = load_sample_data(task, num_samples)\n",
    "            print(f\"Loaded {len(images)} images for {task}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data for {task}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        for model_name in model_list:\n",
    "            print(f\"\\n--- Processing Model: {model_name} ---\")\n",
    "            # Load trained model\n",
    "            model, processor = load_trained_model(task, model_name, input_size)\n",
    "            # Initialize XAI generator\n",
    "            xai_generator = XAIGenerator(model, model_name, input_size)\n",
    "            # Store results for this model\n",
    "            results[task][model_name] = {\n",
    "                'images': images,\n",
    "                'labels': labels,\n",
    "                'heatmaps': []\n",
    "            }\n",
    "            # Process each image with filename\n",
    "            for idx, (image, label, filename) in enumerate(zip(images, labels, filenames)):\n",
    "                print(f\"Processing image {idx+1}/{len(images)} (Label: {label}, File: {filename})\")\n",
    "                # Preprocess image\n",
    "                image_tensor = preprocess_image(image, processor, input_size)\n",
    "                # Generate all heatmaps for this image\n",
    "                heatmaps = xai_generator.generate_all_heatmaps(image_tensor, target_class=label)\n",
    "                results[task][model_name]['heatmaps'].append(heatmaps)\n",
    "                for xai_name, heatmap in heatmaps.items():\n",
    "                    overlay = overlay_heatmap_on_image(image, heatmap)\n",
    "                    # overlay is np.uint8 HxWx3 per implementation\n",
    "                    # Create directory structure: ./heatmap_results/<task_name>/<label_idx>/<image_name>/<baselinemodel>/<XAI>.jpg\n",
    "                    save_dir = Path(\"./heatmap_results\") / task / str(label) / filename / model_name\n",
    "                    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "                    out_path = save_dir / f\"{xai_name}.jpg\"\n",
    "                    try:\n",
    "                        Image.fromarray(overlay).save(out_path, format='JPEG', quality=95)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed to save {out_path}: {e}\")\n",
    "            print(f\"Completed {model_name} for {task}\")\n",
    "            #delete after finish\n",
    "            del xai_generator\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0cc020db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing New Heatmap Generation Function ===\n",
      "This will save heatmaps in the structure: ./heatmap_results/<task_name>/<label_idx>/<image_name>/<baselinemodel>/<XAI>.jpg\n",
      "Starting comprehensive heatmap generation...\n",
      "Tasks: ['DME']\n",
      "Models: ['ResNet-50', 'timm_efficientnet-b4', 'vit-base-patch16-224', 'RETFound_mae']\n",
      "Samples per task: 5\n",
      "\n",
      "=== Processing Task: DME ===\n",
      "Loaded 5 images for DME\n",
      "\n",
      "--- Processing Model: ResNet-50 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f8ddd730af74e449ca1751a2a46c90b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-50 and are newly initialized because the shapes did not match:\n",
      "- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.1.weight: found shape torch.Size([1000, 2048]) in the checkpoint and torch.Size([2, 2048]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume checkpoint /blue/ruogu.fang/tienyuchang/RETFound_MAE/output_dir/DME_binary_all_split-IRB2024_v5-all-microsoft/resnet-50-OCT-bs16ep50lr5e-4optadamw-defaulteval-trsub0--/checkpoint-best.pth for ResNet-50 on DME\n",
      "‚úì GradCAM initialized for ResNet-50 (patch_size: 7)\n",
      "‚úì ScoreCAM initialized for ResNet-50 (patch_size: 7)\n",
      "Masks are loaded.\n",
      "‚úì RISE initialized for ResNet-50 (gpu_batch: 1)\n",
      "‚ö† Attention skipped for ResNet-50 (not a transformer model)\n",
      "Processing image 1/5 (Label: 0, File: 1.2.840.114158.49546168971479733704162688413692655539_latR_13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 264.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated heatmaps: ['GradCAM', 'ScoreCAM']\n",
      "Saved: heatmap_results/DME/0/1.2.840.114158.49546168971479733704162688413692655539_latR_13/ResNet-50/GradCAM.jpg\n",
      "Saved: heatmap_results/DME/0/1.2.840.114158.49546168971479733704162688413692655539_latR_13/ResNet-50/ScoreCAM.jpg\n",
      "Processing image 2/5 (Label: 0, File: 1.2.840.114158.49010168583522380566641094137452482187_latL_13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 251.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated heatmaps: ['GradCAM', 'ScoreCAM']\n",
      "Saved: heatmap_results/DME/0/1.2.840.114158.49010168583522380566641094137452482187_latL_13/ResNet-50/GradCAM.jpg\n",
      "Saved: heatmap_results/DME/0/1.2.840.114158.49010168583522380566641094137452482187_latL_13/ResNet-50/ScoreCAM.jpg\n",
      "Processing image 3/5 (Label: 0, File: 1.2.840.114158.550895777424886690915975404387740230836_latR_13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 265.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated heatmaps: ['GradCAM', 'ScoreCAM']\n",
      "Saved: heatmap_results/DME/0/1.2.840.114158.550895777424886690915975404387740230836_latR_13/ResNet-50/GradCAM.jpg\n",
      "Saved: heatmap_results/DME/0/1.2.840.114158.550895777424886690915975404387740230836_latR_13/ResNet-50/ScoreCAM.jpg\n",
      "Processing image 4/5 (Label: 1, File: 1.2.840.114158.46841810624182637848593001688815960971_latR_10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 267.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated heatmaps: ['GradCAM', 'ScoreCAM']\n",
      "Saved: heatmap_results/DME/1/1.2.840.114158.46841810624182637848593001688815960971_latR_10/ResNet-50/GradCAM.jpg\n",
      "Saved: heatmap_results/DME/1/1.2.840.114158.46841810624182637848593001688815960971_latR_10/ResNet-50/ScoreCAM.jpg\n",
      "Processing image 5/5 (Label: 1, File: 1.2.840.114158.482312393585037452711072409422317557927_latR_13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128/128 [00:00<00:00, 269.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated heatmaps: ['GradCAM', 'ScoreCAM']\n",
      "Saved: heatmap_results/DME/1/1.2.840.114158.482312393585037452711072409422317557927_latR_13/ResNet-50/GradCAM.jpg\n",
      "Saved: heatmap_results/DME/1/1.2.840.114158.482312393585037452711072409422317557927_latR_13/ResNet-50/ScoreCAM.jpg\n",
      "Completed ResNet-50 for DME\n",
      "\n",
      "--- Processing Model: timm_efficientnet-b4 ---\n",
      "Resume checkpoint /blue/ruogu.fang/tienyuchang/RETFound_MAE/output_dir/DME_binary_all_split-IRB2024_v5-all-timm_efficientnet-b4-OCT-bs16ep50lr5e-4optadamw-defaulteval-trsub0--/checkpoint-best.pth for timm_efficientnet-b4 on DME\n",
      "‚úì GradCAM initialized for timm_efficientnet-b4 (patch_size: 7)\n",
      "‚úì ScoreCAM initialized for timm_efficientnet-b4 (patch_size: 7)\n",
      "Masks are loaded.\n",
      "‚úì RISE initialized for timm_efficientnet-b4 (gpu_batch: 1)\n",
      "‚ö† Attention skipped for timm_efficientnet-b4 (not a transformer model)\n",
      "Processing image 1/5 (Label: 0, File: 1.2.840.114158.49546168971479733704162688413692655539_latR_13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:00<00:00, 86.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated heatmaps: ['GradCAM', 'ScoreCAM']\n",
      "Saved: heatmap_results/DME/0/1.2.840.114158.49546168971479733704162688413692655539_latR_13/timm_efficientnet-b4/GradCAM.jpg\n",
      "Saved: heatmap_results/DME/0/1.2.840.114158.49546168971479733704162688413692655539_latR_13/timm_efficientnet-b4/ScoreCAM.jpg\n",
      "Processing image 2/5 (Label: 0, File: 1.2.840.114158.49010168583522380566641094137452482187_latL_13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:00<00:00, 84.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated heatmaps: ['GradCAM', 'ScoreCAM']\n",
      "Saved: heatmap_results/DME/0/1.2.840.114158.49010168583522380566641094137452482187_latL_13/timm_efficientnet-b4/GradCAM.jpg\n",
      "Saved: heatmap_results/DME/0/1.2.840.114158.49010168583522380566641094137452482187_latL_13/timm_efficientnet-b4/ScoreCAM.jpg\n",
      "Processing image 3/5 (Label: 0, File: 1.2.840.114158.550895777424886690915975404387740230836_latR_13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:00<00:00, 86.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated heatmaps: ['GradCAM', 'ScoreCAM']\n",
      "Saved: heatmap_results/DME/0/1.2.840.114158.550895777424886690915975404387740230836_latR_13/timm_efficientnet-b4/GradCAM.jpg\n",
      "Saved: heatmap_results/DME/0/1.2.840.114158.550895777424886690915975404387740230836_latR_13/timm_efficientnet-b4/ScoreCAM.jpg\n",
      "Processing image 4/5 (Label: 1, File: 1.2.840.114158.46841810624182637848593001688815960971_latR_10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:00<00:00, 86.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated heatmaps: ['GradCAM', 'ScoreCAM']\n",
      "Saved: heatmap_results/DME/1/1.2.840.114158.46841810624182637848593001688815960971_latR_10/timm_efficientnet-b4/GradCAM.jpg\n",
      "Saved: heatmap_results/DME/1/1.2.840.114158.46841810624182637848593001688815960971_latR_10/timm_efficientnet-b4/ScoreCAM.jpg\n",
      "Processing image 5/5 (Label: 1, File: 1.2.840.114158.482312393585037452711072409422317557927_latR_13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:00<00:00, 86.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated heatmaps: ['GradCAM', 'ScoreCAM']\n",
      "Saved: heatmap_results/DME/1/1.2.840.114158.482312393585037452711072409422317557927_latR_13/timm_efficientnet-b4/GradCAM.jpg\n",
      "Saved: heatmap_results/DME/1/1.2.840.114158.482312393585037452711072409422317557927_latR_13/timm_efficientnet-b4/ScoreCAM.jpg\n",
      "Completed timm_efficientnet-b4 for DME\n",
      "\n",
      "--- Processing Model: vit-base-patch16-224 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc1dcd40111b4fa9881857cd3a582968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume checkpoint /blue/ruogu.fang/tienyuchang/RETFound_MAE/output_dir/DME_binary_all_split-IRB2024_v5-all-google/vit-base-patch16-224-in21k-OCT-bs16ep50lr5e-4optadamw-defaulteval-trsub0--/checkpoint-best.pth for vit-base-patch16-224 on DME\n",
      "‚úì GradCAM initialized for vit-base-patch16-224 (patch_size: 16)\n",
      "‚úì ScoreCAM initialized for vit-base-patch16-224 (patch_size: 16)\n",
      "Masks are loaded.\n",
      "‚úì RISE initialized for vit-base-patch16-224 (gpu_batch: 1)\n",
      "‚úì Attention initialized for vit-base-patch16-224 (layers: 12)\n",
      "Processing image 1/5 (Label: 0, File: 1.2.840.114158.49546168971479733704162688413692655539_latR_13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:00<00:00, 52.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated heatmaps: ['GradCAM', 'ScoreCAM', 'Attention']\n",
      "Saved: heatmap_results/DME/0/1.2.840.114158.49546168971479733704162688413692655539_latR_13/vit-base-patch16-224/GradCAM.jpg\n",
      "Saved: heatmap_results/DME/0/1.2.840.114158.49546168971479733704162688413692655539_latR_13/vit-base-patch16-224/ScoreCAM.jpg\n",
      "Saved: heatmap_results/DME/0/1.2.840.114158.49546168971479733704162688413692655539_latR_13/vit-base-patch16-224/Attention.jpg\n",
      "Processing image 2/5 (Label: 0, File: 1.2.840.114158.49010168583522380566641094137452482187_latL_13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:00<00:00, 52.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated heatmaps: ['GradCAM', 'ScoreCAM', 'Attention']\n",
      "Saved: heatmap_results/DME/0/1.2.840.114158.49010168583522380566641094137452482187_latL_13/vit-base-patch16-224/GradCAM.jpg\n",
      "Saved: heatmap_results/DME/0/1.2.840.114158.49010168583522380566641094137452482187_latL_13/vit-base-patch16-224/ScoreCAM.jpg\n",
      "Saved: heatmap_results/DME/0/1.2.840.114158.49010168583522380566641094137452482187_latL_13/vit-base-patch16-224/Attention.jpg\n",
      "Processing image 3/5 (Label: 0, File: 1.2.840.114158.550895777424886690915975404387740230836_latR_13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:00<00:00, 53.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated heatmaps: ['GradCAM', 'ScoreCAM', 'Attention']\n",
      "Saved: heatmap_results/DME/0/1.2.840.114158.550895777424886690915975404387740230836_latR_13/vit-base-patch16-224/GradCAM.jpg\n",
      "Saved: heatmap_results/DME/0/1.2.840.114158.550895777424886690915975404387740230836_latR_13/vit-base-patch16-224/ScoreCAM.jpg\n",
      "Saved: heatmap_results/DME/0/1.2.840.114158.550895777424886690915975404387740230836_latR_13/vit-base-patch16-224/Attention.jpg\n",
      "Processing image 4/5 (Label: 1, File: 1.2.840.114158.46841810624182637848593001688815960971_latR_10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:00<00:00, 53.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated heatmaps: ['GradCAM', 'ScoreCAM', 'Attention']\n",
      "Saved: heatmap_results/DME/1/1.2.840.114158.46841810624182637848593001688815960971_latR_10/vit-base-patch16-224/GradCAM.jpg\n",
      "Saved: heatmap_results/DME/1/1.2.840.114158.46841810624182637848593001688815960971_latR_10/vit-base-patch16-224/ScoreCAM.jpg\n",
      "Saved: heatmap_results/DME/1/1.2.840.114158.46841810624182637848593001688815960971_latR_10/vit-base-patch16-224/Attention.jpg\n",
      "Processing image 5/5 (Label: 1, File: 1.2.840.114158.482312393585037452711072409422317557927_latR_13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:00<00:00, 52.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated heatmaps: ['GradCAM', 'ScoreCAM', 'Attention']\n",
      "Saved: heatmap_results/DME/1/1.2.840.114158.482312393585037452711072409422317557927_latR_13/vit-base-patch16-224/GradCAM.jpg\n",
      "Saved: heatmap_results/DME/1/1.2.840.114158.482312393585037452711072409422317557927_latR_13/vit-base-patch16-224/ScoreCAM.jpg\n",
      "Saved: heatmap_results/DME/1/1.2.840.114158.482312393585037452711072409422317557927_latR_13/vit-base-patch16-224/Attention.jpg\n",
      "Completed vit-base-patch16-224 for DME\n",
      "\n",
      "--- Processing Model: RETFound_mae ---\n",
      "Resume checkpoint /blue/ruogu.fang/tienyuchang/RETFound_MAE/output_dir/DME_binary_all_split-IRB2024_v5-all-RETFound_mae_natureOCT-OCT-bs16ep50lr5e-4optadamw-roc_auceval-trsub0--/checkpoint-best.pth for RETFound_mae on DME\n",
      "‚úì GradCAM initialized for RETFound_mae (patch_size: 16)\n",
      "‚úì ScoreCAM initialized for RETFound_mae (patch_size: 16)\n",
      "Masks are loaded.\n",
      "‚úì RISE initialized for RETFound_mae (gpu_batch: 1)\n",
      "‚úó Failed to initialize Attention for RETFound_mae: symbolically traced variables cannot be used as inputs to control flow\n",
      "  Note: This may be due to symbolic tracing issues with the model architecture\n",
      "  üîÑ Trying alternative attention extraction for RETFound...\n",
      "‚úì Alternative attention method initialized for RETFound_mae\n",
      "Processing image 1/5 (Label: 0, File: 1.2.840.114158.49546168971479733704162688413692655539_latR_13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:03<00:00, 18.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated heatmaps: ['GradCAM', 'ScoreCAM', 'Attention']\n",
      "Saved: heatmap_results/DME/0/1.2.840.114158.49546168971479733704162688413692655539_latR_13/RETFound_mae/GradCAM.jpg\n",
      "Saved: heatmap_results/DME/0/1.2.840.114158.49546168971479733704162688413692655539_latR_13/RETFound_mae/ScoreCAM.jpg\n",
      "Saved: heatmap_results/DME/0/1.2.840.114158.49546168971479733704162688413692655539_latR_13/RETFound_mae/Attention.jpg\n",
      "Processing image 2/5 (Label: 0, File: 1.2.840.114158.49010168583522380566641094137452482187_latL_13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:03<00:00, 20.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated heatmaps: ['GradCAM', 'ScoreCAM', 'Attention']\n",
      "Saved: heatmap_results/DME/0/1.2.840.114158.49010168583522380566641094137452482187_latL_13/RETFound_mae/GradCAM.jpg\n",
      "Saved: heatmap_results/DME/0/1.2.840.114158.49010168583522380566641094137452482187_latL_13/RETFound_mae/ScoreCAM.jpg\n",
      "Saved: heatmap_results/DME/0/1.2.840.114158.49010168583522380566641094137452482187_latL_13/RETFound_mae/Attention.jpg\n",
      "Processing image 3/5 (Label: 0, File: 1.2.840.114158.550895777424886690915975404387740230836_latR_13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:03<00:00, 19.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated heatmaps: ['GradCAM', 'ScoreCAM', 'Attention']\n",
      "Saved: heatmap_results/DME/0/1.2.840.114158.550895777424886690915975404387740230836_latR_13/RETFound_mae/GradCAM.jpg\n",
      "Saved: heatmap_results/DME/0/1.2.840.114158.550895777424886690915975404387740230836_latR_13/RETFound_mae/ScoreCAM.jpg\n",
      "Saved: heatmap_results/DME/0/1.2.840.114158.550895777424886690915975404387740230836_latR_13/RETFound_mae/Attention.jpg\n",
      "Processing image 4/5 (Label: 1, File: 1.2.840.114158.46841810624182637848593001688815960971_latR_10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:03<00:00, 19.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated heatmaps: ['GradCAM', 'ScoreCAM', 'Attention']\n",
      "Saved: heatmap_results/DME/1/1.2.840.114158.46841810624182637848593001688815960971_latR_10/RETFound_mae/GradCAM.jpg\n",
      "Saved: heatmap_results/DME/1/1.2.840.114158.46841810624182637848593001688815960971_latR_10/RETFound_mae/ScoreCAM.jpg\n",
      "Saved: heatmap_results/DME/1/1.2.840.114158.46841810624182637848593001688815960971_latR_10/RETFound_mae/Attention.jpg\n",
      "Processing image 5/5 (Label: 1, File: 1.2.840.114158.482312393585037452711072409422317557927_latR_13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:03<00:00, 19.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated heatmaps: ['GradCAM', 'ScoreCAM', 'Attention']\n",
      "Saved: heatmap_results/DME/1/1.2.840.114158.482312393585037452711072409422317557927_latR_13/RETFound_mae/GradCAM.jpg\n",
      "Saved: heatmap_results/DME/1/1.2.840.114158.482312393585037452711072409422317557927_latR_13/RETFound_mae/ScoreCAM.jpg\n",
      "Saved: heatmap_results/DME/1/1.2.840.114158.482312393585037452711072409422317557927_latR_13/RETFound_mae/Attention.jpg\n",
      "Completed RETFound_mae for DME\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the new function with improved directory structure\n",
    "print(\"=== Testing New Heatmap Generation Function ===\")\n",
    "print(\"This will save heatmaps in the structure: ./heatmap_results/<task_name>/<label_idx>/<image_name>/<baselinemodel>/<XAI>.jpg\")\n",
    "\n",
    "# Test with a small number of samples first\n",
    "#Model_list = ['ResNet-50', 'timm_efficientnet-b4', 'vit-base-patch16-224', 'RETFound_mae']\n",
    "N_samples = 5\n",
    "heatmap_results_v2 = generate_comprehensive_heatmaps_v2(num_samples=N_samples)  # Start with 3 samples for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da2512c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating visualization for DME...\n",
      "Heatmap grid saved to heatmap_results/DME/heatmap_grid.png\n",
      "Heatmap grid saved to heatmap_results/DME/heatmap_grid.png\n",
      "Heatmap grid saved to heatmap_results/DME/heatmap_grid.png\n",
      "Heatmap grid saved to heatmap_results/DME/heatmap_grid.png\n",
      "Heatmap grid saved to heatmap_results/DME/heatmap_grid.png\n"
     ]
    }
   ],
   "source": [
    "# Create individual task-specific visualizations\n",
    "def create_task_specific_grids(results_dict, output_dir, num_samples):\n",
    "    \"\"\"Create separate visualizations for each task\"\"\"\n",
    "    for task in results_dict.keys():\n",
    "        print(f\"Creating visualization for {task}...\")\n",
    "        # Create task-specific results dict\n",
    "        task_results = {task: results_dict[task]}\n",
    "        # Create visualization\n",
    "        task_grid_path = output_dir / task / \"heatmap_grid.png\"\n",
    "        create_heatmap_grid(task_results, task, num_samples=num_samples, save_path=task_grid_path)\n",
    "\n",
    "# Generate task-specific grids\n",
    "output_dir = Path(\"./heatmap_results\") \n",
    "create_task_specific_grids(heatmap_results_v2, output_dir, N_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37a37cf-fb8c-4a2c-90fb-1fa83f32edf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "octxai",
   "language": "python",
   "name": "octxai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
