{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbb5203-5c25-46e9-9800-69fda108889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "from pytorch_grad_cam import run_dff_on_image, GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from typing import List, Callable, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088415c1-dba0-4047-aee5-53bc1fabac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    ViTImageProcessor, ViTForImageClassification,\n",
    "    AutoImageProcessor, EfficientNetForImageClassification,\n",
    "    ResNetForImageClassification, AutoModel\n",
    ")\n",
    "import models_vit as models\n",
    "from util.datasets import TransformWrapper\n",
    "\n",
    "#get model\n",
    "def get_model(task,model,input_size,nb_classes):\n",
    "    if 'ADCon' in task:\n",
    "        id2label = {0: \"control\", 1: \"ad\"}\n",
    "        label2id = {v: k for k, v in id2label.items()}\n",
    "    else:\n",
    "        id2label = {i: f\"class_{i}\" for i in range(nb_classes)}\n",
    "        label2id = {v: k for k, v in id2label.items()}\n",
    "    processor = None\n",
    "    if 'RETFound_mae' in model:\n",
    "        model = models.__dict__['RETFound_mae'](\n",
    "        img_size=input_size,\n",
    "        num_classes=nb_classes,\n",
    "        drop_path_rate=0.2,\n",
    "        global_pool=True,\n",
    "    )\n",
    "    elif 'vit-base-patch16-224' in model:\n",
    "            # ViT-base-patch16-224 preprocessor\n",
    "            model_ = 'google/vit-base-patch16-224'\n",
    "            processor = TransformWrapper(ViTImageProcessor.from_pretrained(model_))\n",
    "            model = ViTForImageClassification.from_pretrained(\n",
    "                model_,\n",
    "                image_size=input_size, #Not in tianhao code, default 224\n",
    "                num_labels=nb_classes,\n",
    "                hidden_dropout_prob=0.0, #Not in tianhao code, default 0.0\n",
    "                attention_probs_dropout_prob=0.0, #Not in tianhao code, default 0.0\n",
    "                id2label=id2label,\n",
    "                label2id=label2id,\n",
    "                ignore_mismatched_sizes=True\n",
    "            )\n",
    "    elif 'efficientnet-b4' in model:\n",
    "        # EfficientNet-B4 preprocessor\n",
    "        model_ = 'google/efficientnet-b4'\n",
    "        processor = TransformWrapper(AutoImageProcessor.from_pretrained(model_))\n",
    "        model = EfficientNetForImageClassification.from_pretrained(\n",
    "            model_,\n",
    "            image_size=input_size,\n",
    "            num_labels=nb_classes,\n",
    "            dropout_rate=0.0,\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "    elif 'resnet-50' in model:\n",
    "        model_name = 'microsoft/resnet-50'\n",
    "        processor = TransformWrapper(AutoImageProcessor.from_pretrained(model_name))\n",
    "        model = ResNetForImageClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=nb_classes,\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "\n",
    "    return model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94f0a85-078e-4a03-a4f7-57bf8778c49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task and dataset\n",
    "Task_list = ['ADCon','DME']\n",
    "dataset_fname = 'sampled_labels01.csv'\n",
    "dataset_dir = '/blue/ruogu.fang/tienyuchang/OCT_EDA'\n",
    "img_p_fmt = \"label_%d/%s\" #label index and oct_img name\n",
    "\n",
    "# model\n",
    "Model_list = ['ResNet-50', 'efficientnet-b4', 'vit-base-patch16-224', 'RETFound_mae']\n",
    "ADCon_finetuned = [\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"ad_control_detect_data-IRB2024v5_ADCON_DL_data-all-RETFound_mae-OCT-defaulteval---bal_sampler-/\"\n",
    "]\n",
    "DME_finetuned = [\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"DME_binary_all_split-IRB2024_v5-all-google/vit-base-patch16-224-in21k-OCT-bs16ep50lr5e-4optadamw-defaulteval-trsub0--/\",\n",
    "    \"DME_binary_all_split-IRB2024_v5-all-RETFound_mae_natureOCT-OCT-bs16ep50lr5e-4optadamw-roc_auceval-trsub0--/\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db5def6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import XAI methods\n",
    "from baselines.GradCAM_v2 import PytorchCAM\n",
    "from baselines.RISE import RISEBatch\n",
    "from baselines.Attention import Attention_Map\n",
    "from baselines.ViT_explanation_generator import LRP, Baselines\n",
    "from pytorch_grad_cam import GradCAM, ScoreCAM\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3009084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and preprocessing functions\n",
    "def load_sample_data(task, dataset_dir, dataset_fname, num_samples=5):\n",
    "    \"\"\"Load sample images for a given task\"\"\"\n",
    "    df = pd.read_csv(os.path.join(dataset_dir, dataset_fname))\n",
    "    \n",
    "    if task == 'ADCon':\n",
    "        # Filter for AD and Control samples\n",
    "        task_df = df[df['label'].isin([0, 1])]  # 0: control, 1: ad\n",
    "    elif task == 'DME':\n",
    "        # Filter for DME samples (assuming binary classification)\n",
    "        task_df = df[df['label'].isin([0, 1])]  # Adjust based on actual DME labels\n",
    "    \n",
    "    # Sample random images\n",
    "    sample_df = task_df\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for _, row in sample_df.iterrows():\n",
    "        # Extract just the filename from oct_img\n",
    "        filename = os.path.basename(row['oct_img']) if isinstance(row['oct_img'], str) else row['oct_img']\n",
    "        img_path = os.path.join(dataset_dir, img_p_fmt % (row['label'], filename))\n",
    "        if os.path.exists(img_path):\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                images.append(img)\n",
    "                labels.append(row['label'])\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_path}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "def preprocess_image(image, processor=None, input_size=224):\n",
    "    \"\"\"Preprocess image for model input\"\"\"\n",
    "    if processor is not None:\n",
    "        # Use model-specific processor\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        return inputs['pixel_values']\n",
    "    else:\n",
    "        # Default preprocessing for RETFound\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((input_size, input_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        return transform(image).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdfdc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained models function\n",
    "def load_trained_model(task, model_name, input_size=224, nb_classes=2):\n",
    "    \"\"\"Load a trained model for a specific task\"\"\"\n",
    "    model, processor = get_model(task, model_name, input_size, nb_classes)\n",
    "    \n",
    "    # Load model weights based on task and model\n",
    "    if task == 'ADCon':\n",
    "        model_paths = ADCon_finetuned\n",
    "    elif task == 'DME':\n",
    "        model_paths = DME_finetuned\n",
    "    else:\n",
    "        print(f\"Unknown task: {task}\")\n",
    "        model.eval()\n",
    "        return model, processor\n",
    "    \n",
    "    model_idx = Model_list.index(model_name)\n",
    "    model_path = model_paths[model_idx]\n",
    "    \n",
    "    # Load finetuned model if specified (following main_XAI_evaluation.py pattern)\n",
    "    if model_path and model_path != '' and model_path != '0':\n",
    "        if os.path.exists(model_path):\n",
    "            try:\n",
    "                # Load checkpoint\n",
    "                if model_path.startswith('https'):\n",
    "                    checkpoint = torch.hub.load_state_dict_from_url(\n",
    "                        model_path, map_location='cpu', check_hash=True)\n",
    "                else:\n",
    "                    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "                \n",
    "                # Extract model state dict\n",
    "                if 'model' in checkpoint:\n",
    "                    checkpoint_model = checkpoint['model']\n",
    "                else:\n",
    "                    checkpoint_model = checkpoint\n",
    "                \n",
    "                # Load with strict=False to handle potential mismatches\n",
    "                model.load_state_dict(checkpoint_model, strict=False)\n",
    "                print(f\"Resume checkpoint {model_path} for {model_name} on {task}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading model {model_name} for {task}: {e}\")\n",
    "                print(\"Using pretrained weights instead\")\n",
    "        else:\n",
    "            print(f\"Model path not found: {model_path}\")\n",
    "            print(f\"Using pretrained weights for {model_name} on {task}\")\n",
    "    else:\n",
    "        print(f\"No checkpoint specified for {model_name} on {task}, using pretrained weights\")\n",
    "    \n",
    "    model.eval()\n",
    "    return model, processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00dd935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XAI Methods Implementation\n",
    "class XAIGenerator:\n",
    "    def __init__(self, model, model_name, input_size=224):\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.input_size = input_size\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # Initialize XAI methods\n",
    "        self.init_xai_methods()\n",
    "    \n",
    "    def get_model_specific_config(self):\n",
    "        \"\"\"Get model-specific configuration for XAI methods\"\"\"\n",
    "        config = {\n",
    "            'patch_size': 14,\n",
    "            'gpu_batch': 50,\n",
    "            'attention_layers': 12\n",
    "        }\n",
    "        \n",
    "        # Model-specific configurations\n",
    "        if 'resnet' in self.model_name.lower():\n",
    "            config.update({\n",
    "                'patch_size': 7,  # ResNet has different spatial resolution\n",
    "                'gpu_batch': 100,  # ResNet can handle larger batches\n",
    "                'target_layer_hint': 'layer4'  # ResNet's final conv layer\n",
    "            })\n",
    "        elif 'efficientnet' in self.model_name.lower():\n",
    "            config.update({\n",
    "                'patch_size': 7,  # EfficientNet spatial resolution\n",
    "                'gpu_batch': 75,\n",
    "                'target_layer_hint': 'features'  # EfficientNet's feature extractor\n",
    "            })\n",
    "        elif 'vit' in self.model_name.lower():\n",
    "            config.update({\n",
    "                'patch_size': 16,  # ViT patch size\n",
    "                'gpu_batch': 50,\n",
    "                'attention_layers': 12,  # Standard ViT-Base layers\n",
    "                'target_layer_hint': 'encoder'  # ViT encoder layers\n",
    "            })\n",
    "        elif 'retfound' in self.model_name.lower():\n",
    "            config.update({\n",
    "                'patch_size': 16,  # RETFound uses ViT architecture\n",
    "                'gpu_batch': 50,\n",
    "                'attention_layers': 12,\n",
    "                'target_layer_hint': 'blocks'  # RETFound blocks\n",
    "            })\n",
    "        \n",
    "        return config\n",
    "    \n",
    "    def init_xai_methods(self):\n",
    "        \"\"\"Initialize all XAI methods with model-specific configurations\"\"\"\n",
    "        config = self.get_model_specific_config()\n",
    "        \n",
    "        try:\n",
    "            # GradCAM with model-specific config\n",
    "            self.gradcam = PytorchCAM(\n",
    "                self.model, \n",
    "                self.model_name, \n",
    "                self.input_size, \n",
    "                patch_size=config['patch_size'],\n",
    "                method=GradCAM\n",
    "            )\n",
    "            print(f\"âœ“ GradCAM initialized for {self.model_name} (patch_size: {config['patch_size']})\")\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Failed to initialize GradCAM for {self.model_name}: {e}\")\n",
    "            self.gradcam = None\n",
    "        \n",
    "        try:\n",
    "            # ScoreCAM with model-specific config\n",
    "            self.scorecam = PytorchCAM(\n",
    "                self.model, \n",
    "                self.model_name, \n",
    "                self.input_size, \n",
    "                patch_size=config['patch_size'],\n",
    "                method=ScoreCAM\n",
    "            )\n",
    "            print(f\"âœ“ ScoreCAM initialized for {self.model_name} (patch_size: {config['patch_size']})\")\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Failed to initialize ScoreCAM for {self.model_name}: {e}\")\n",
    "            self.scorecam = None\n",
    "        \n",
    "        try:\n",
    "            # RISE with model-specific batch size\n",
    "            self.rise = RISEBatch(\n",
    "                self.model, \n",
    "                input_size=(self.input_size, self.input_size), \n",
    "                gpu_batch=config['gpu_batch']\n",
    "            )\n",
    "            print(f\"âœ“ RISE initialized for {self.model_name} (gpu_batch: {config['gpu_batch']})\")\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Failed to initialize RISE for {self.model_name}: {e}\")\n",
    "            self.rise = None\n",
    "        \n",
    "        try:\n",
    "            # Attention Maps (only for ViT-based models)\n",
    "            if 'vit' in self.model_name.lower() or 'retfound' in self.model_name.lower():\n",
    "                self.attention = Attention_Map(\n",
    "                    self.model, \n",
    "                    self.model_name, \n",
    "                    self.input_size, \n",
    "                    N=config['attention_layers'],\n",
    "                    use_rollout=True\n",
    "                )\n",
    "                print(f\"âœ“ Attention initialized for {self.model_name} (layers: {config['attention_layers']})\")\n",
    "            else:\n",
    "                self.attention = None\n",
    "                print(f\"âš  Attention skipped for {self.model_name} (not a transformer model)\")\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Failed to initialize Attention for {self.model_name}: {e}\")\n",
    "            self.attention = None\n",
    "        \n",
    "        # LRP (requires special model implementation)\n",
    "        self.lrp = None  # Will implement if model supports it\n",
    "        \n",
    "        # Print target layer information\n",
    "        self.print_target_layer_info()\n",
    "    \n",
    "    def print_target_layer_info(self):\n",
    "        \"\"\"Print information about target layers used for each XAI method\"\"\"\n",
    "        print(f\"\\nðŸ“‹ Target Layer Information for {self.model_name}:\")\n",
    "        \n",
    "        # Get target layer for CAM methods\n",
    "        try:\n",
    "            if 'retfound' in self.model_name.lower():\n",
    "                target_layer = self.model.blocks[-1]\n",
    "                print(f\"  ðŸŽ¯ CAM Methods: model.blocks[-1] (Final transformer block)\")\n",
    "            elif 'vit' in self.model_name.lower():\n",
    "                if hasattr(self.model, 'vit'):\n",
    "                    target_layer = self.model.vit.encoder.layer[-1]\n",
    "                    print(f\"  ðŸŽ¯ CAM Methods: model.vit.encoder.layer[-1] (Final encoder layer)\")\n",
    "                elif hasattr(self.model, 'base_model'):\n",
    "                    target_layer = self.model.base_model.vit.encoder.layer[-1]\n",
    "                    print(f\"  ðŸŽ¯ CAM Methods: model.base_model.vit.encoder.layer[-1] (Final encoder layer)\")\n",
    "            elif 'resnet' in self.model_name.lower():\n",
    "                target_layer = self.model.resnet.encoder.stages[-1].layers[-1]\n",
    "                print(f\"  ðŸŽ¯ CAM Methods: model.resnet.encoder.stages[-1].layers[-1] (Final ResNet stage)\")\n",
    "            elif 'efficientnet' in self.model_name.lower():\n",
    "                if hasattr(self.model, 'efficientnet'):\n",
    "                    target_layer = self.model.efficientnet.encoder.blocks[-1]\n",
    "                    print(f\"  ðŸŽ¯ CAM Methods: model.efficientnet.encoder.blocks[-1] (Final EfficientNet block)\")\n",
    "                else:\n",
    "                    print(f\"  ðŸŽ¯ CAM Methods: Auto-resolved target layer\")\n",
    "            \n",
    "            print(f\"  ðŸ” RISE: Uses entire model with masking\")\n",
    "            \n",
    "            if self.attention is not None:\n",
    "                print(f\"  ðŸ‘ Attention: Uses all transformer layers with rollout\")\n",
    "            else:\n",
    "                print(f\"  ðŸ‘ Attention: Not applicable (non-transformer model)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  âš  Could not determine exact target layer: {e}\")\n",
    "            print(f\"  ðŸŽ¯ CAM Methods: Using auto-resolved target layer\")\n",
    "    \n",
    "    def generate_gradcam(self, image_tensor, target_class=None):\n",
    "        \"\"\"Generate GradCAM heatmap\"\"\"\n",
    "        if self.gradcam is None:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            image_tensor = image_tensor.to(self.device)\n",
    "            \n",
    "            if target_class is None:\n",
    "                # Get predicted class\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(image_tensor)\n",
    "                    target_class = outputs.argmax(dim=1).item()\n",
    "            \n",
    "            targets = [ClassifierOutputTarget(target_class)]\n",
    "            heatmap = self.gradcam(image_tensor, targets)\n",
    "            return heatmap[0] if len(heatmap) > 0 else None\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating GradCAM: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def generate_scorecam(self, image_tensor, target_class=None):\n",
    "        \"\"\"Generate ScoreCAM heatmap\"\"\"\n",
    "        if self.scorecam is None:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            image_tensor = image_tensor.to(self.device)\n",
    "            \n",
    "            if target_class is None:\n",
    "                # Get predicted class\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(image_tensor)\n",
    "                    target_class = outputs.argmax(dim=1).item()\n",
    "            \n",
    "            targets = [ClassifierOutputTarget(target_class)]\n",
    "            heatmap = self.scorecam(image_tensor, targets)\n",
    "            return heatmap[0] if len(heatmap) > 0 else None\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating ScoreCAM: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def generate_rise(self, image_tensor, target_class=None):\n",
    "        \"\"\"Generate RISE heatmap\"\"\"\n",
    "        if self.rise is None:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            image_tensor = image_tensor.to(self.device)\n",
    "            \n",
    "            if target_class is None:\n",
    "                # Get predicted class\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(image_tensor)\n",
    "                    target_class = outputs.argmax(dim=1).item()\n",
    "            \n",
    "            heatmaps = self.rise(image_tensor)\n",
    "            return heatmaps[0, target_class] if heatmaps is not None else None\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating RISE: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def generate_attention(self, image_tensor):\n",
    "        \"\"\"Generate Attention heatmap\"\"\"\n",
    "        if self.attention is None:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            image_tensor = image_tensor.to(self.device)\n",
    "            attention_map = self.attention(image_tensor)\n",
    "            return attention_map[0] if attention_map is not None else None\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating Attention: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def generate_all_heatmaps(self, image_tensor, target_class=None):\n",
    "        \"\"\"Generate all available heatmaps for an image\"\"\"\n",
    "        heatmaps = {}\n",
    "        \n",
    "        # GradCAM\n",
    "        gradcam_map = self.generate_gradcam(image_tensor, target_class)\n",
    "        if gradcam_map is not None:\n",
    "            heatmaps['GradCAM'] = gradcam_map\n",
    "        \n",
    "        # ScoreCAM\n",
    "        scorecam_map = self.generate_scorecam(image_tensor, target_class)\n",
    "        if scorecam_map is not None:\n",
    "            heatmaps['ScoreCAM'] = scorecam_map\n",
    "        \n",
    "        # RISE\n",
    "        rise_map = self.generate_rise(image_tensor, target_class)\n",
    "        if rise_map is not None:\n",
    "            heatmaps['RISE'] = rise_map\n",
    "        \n",
    "        # Attention\n",
    "        attention_map = self.generate_attention(image_tensor)\n",
    "        if attention_map is not None:\n",
    "            heatmaps['Attention'] = attention_map\n",
    "        \n",
    "        return heatmaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063fabda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization functions\n",
    "def normalize_heatmap(heatmap):\n",
    "    \"\"\"Normalize heatmap to 0-1 range\"\"\"\n",
    "    if heatmap is None:\n",
    "        return None\n",
    "    \n",
    "    heatmap = np.array(heatmap)\n",
    "    if heatmap.max() == heatmap.min():\n",
    "        return np.zeros_like(heatmap)\n",
    "    \n",
    "    return (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
    "\n",
    "def overlay_heatmap_on_image(image, heatmap, alpha=0.4, colormap='jet'):\n",
    "    \"\"\"Overlay heatmap on original image\"\"\"\n",
    "    if heatmap is None:\n",
    "        return np.array(image)\n",
    "    \n",
    "    # Normalize heatmap\n",
    "    heatmap_norm = normalize_heatmap(heatmap)\n",
    "    \n",
    "    # Resize heatmap to match image size\n",
    "    if isinstance(image, Image.Image):\n",
    "        image_array = np.array(image)\n",
    "        image_size = image.size\n",
    "    else:\n",
    "        image_array = image\n",
    "        image_size = (image.shape[1], image.shape[0])\n",
    "    \n",
    "    heatmap_resized = cv2.resize(heatmap_norm, image_size)\n",
    "    \n",
    "    # Apply colormap\n",
    "    cmap = plt.get_cmap(colormap)\n",
    "    heatmap_colored = cmap(heatmap_resized)[:, :, :3]  # Remove alpha channel\n",
    "    \n",
    "    # Normalize image\n",
    "    image_norm = image_array.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Overlay\n",
    "    overlay = alpha * heatmap_colored + (1 - alpha) * image_norm\n",
    "    overlay = np.clip(overlay, 0, 1)\n",
    "    \n",
    "    return (overlay * 255).astype(np.uint8)\n",
    "\n",
    "def create_heatmap_grid(results_dict, save_path=None):\n",
    "    \"\"\"Create a comprehensive grid visualization of all heatmaps\"\"\"\n",
    "    tasks = list(results_dict.keys())\n",
    "    models = Model_list\n",
    "    xai_methods = ['GradCAM', 'ScoreCAM', 'RISE', 'Attention']\n",
    "    \n",
    "    # Calculate grid dimensions\n",
    "    n_tasks = len(tasks)\n",
    "    n_models = len(models)\n",
    "    n_methods = len(xai_methods)\n",
    "    n_samples = len(results_dict[tasks[0]][models[0]]['images']) if tasks and models else 0\n",
    "    \n",
    "    # Create figure\n",
    "    fig_width = max(20, n_methods * 4)\n",
    "    fig_height = max(15, n_tasks * n_models * 3)\n",
    "    fig, axes = plt.subplots(\n",
    "        n_tasks * n_models * n_samples, \n",
    "        n_methods + 1,  # +1 for original image\n",
    "        figsize=(fig_width, fig_height)\n",
    "    )\n",
    "    \n",
    "    if n_tasks * n_models * n_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    elif len(axes.shape) == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    row_idx = 0\n",
    "    \n",
    "    for task_idx, task in enumerate(tasks):\n",
    "        for model_idx, model in enumerate(models):\n",
    "            if model not in results_dict[task]:\n",
    "                continue\n",
    "                \n",
    "            model_results = results_dict[task][model]\n",
    "            images = model_results['images']\n",
    "            labels = model_results['labels']\n",
    "            heatmaps = model_results['heatmaps']\n",
    "            \n",
    "            for sample_idx in range(len(images)):\n",
    "                image = images[sample_idx]\n",
    "                label = labels[sample_idx]\n",
    "                sample_heatmaps = heatmaps[sample_idx]\n",
    "                \n",
    "                # Original image\n",
    "                axes[row_idx, 0].imshow(image)\n",
    "                axes[row_idx, 0].set_title(f'{task}-{model}\\nSample {sample_idx+1} (Label: {label})')\n",
    "                axes[row_idx, 0].axis('off')\n",
    "                \n",
    "                # Heatmaps\n",
    "                for method_idx, method in enumerate(xai_methods):\n",
    "                    col_idx = method_idx + 1\n",
    "                    \n",
    "                    if method in sample_heatmaps and sample_heatmaps[method] is not None:\n",
    "                        overlay = overlay_heatmap_on_image(image, sample_heatmaps[method])\n",
    "                        axes[row_idx, col_idx].imshow(overlay)\n",
    "                        axes[row_idx, col_idx].set_title(f'{method}')\n",
    "                    else:\n",
    "                        axes[row_idx, col_idx].text(0.5, 0.5, 'N/A', \n",
    "                                                  ha='center', va='center', \n",
    "                                                  transform=axes[row_idx, col_idx].transAxes)\n",
    "                        axes[row_idx, col_idx].set_title(f'{method}')\n",
    "                    \n",
    "                    axes[row_idx, col_idx].axis('off')\n",
    "                \n",
    "                row_idx += 1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Heatmap grid saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90136eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution: Generate heatmaps for all combinations\n",
    "def generate_comprehensive_heatmaps(num_samples=3):\n",
    "    \"\"\"Generate heatmaps for all task-model combinations\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    input_size = 224\n",
    "    \n",
    "    print(\"Starting comprehensive heatmap generation...\")\n",
    "    print(f\"Tasks: {Task_list}\")\n",
    "    print(f\"Models: {Model_list}\")\n",
    "    print(f\"Samples per task: {num_samples}\")\n",
    "    \n",
    "    for task in Task_list:\n",
    "        print(f\"\\n=== Processing Task: {task} ===\")\n",
    "        results[task] = {}\n",
    "        \n",
    "        # Load sample data for this task\n",
    "        try:\n",
    "            images, labels = load_sample_data(task, dataset_dir, dataset_fname, num_samples)\n",
    "            print(f\"Loaded {len(images)} images for {task}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data for {task}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        for model_name in Model_list:\n",
    "            print(f\"\\n--- Processing Model: {model_name} ---\")\n",
    "            \n",
    "            try:\n",
    "                # Load trained model\n",
    "                model, processor = load_trained_model(task, model_name, input_size)\n",
    "                \n",
    "                # Initialize XAI generator\n",
    "                xai_generator = XAIGenerator(model, model_name, input_size)\n",
    "                \n",
    "                # Store results for this model\n",
    "                results[task][model_name] = {\n",
    "                    'images': images,\n",
    "                    'labels': labels,\n",
    "                    'heatmaps': []\n",
    "                }\n",
    "                \n",
    "                # Process each image\n",
    "                for idx, (image, label) in enumerate(zip(images, labels)):\n",
    "                    print(f\"Processing image {idx+1}/{len(images)} (Label: {label})\")\n",
    "                    \n",
    "                    # Preprocess image\n",
    "                    image_tensor = preprocess_image(image, processor, input_size)\n",
    "                    \n",
    "                    # Generate all heatmaps for this image\n",
    "                    heatmaps = xai_generator.generate_all_heatmaps(image_tensor, target_class=label)\n",
    "                    \n",
    "                    results[task][model_name]['heatmaps'].append(heatmaps)\n",
    "                    \n",
    "                    print(f\"Generated heatmaps: {list(heatmaps.keys())}\")\n",
    "                \n",
    "                print(f\"Completed {model_name} for {task}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {model_name} for {task}: {e}\")\n",
    "                results[task][model_name] = {\n",
    "                    'images': images,\n",
    "                    'labels': labels,\n",
    "                    'heatmaps': [{}] * len(images)\n",
    "                }\n",
    "                continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Print model-specific configuration summary\n",
    "def print_model_config_summary():\n",
    "    \"\"\"Print a summary of model-specific configurations\"\"\"\n",
    "    print(\"\\nðŸ“Š MODEL-SPECIFIC CONFIGURATION SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    configs = []\n",
    "    for model_name in Model_list:\n",
    "        # Create temporary XAI generator to get config\n",
    "        dummy_model = torch.nn.Identity()  # Placeholder\n",
    "        temp_generator = XAIGenerator.__new__(XAIGenerator)\n",
    "        temp_generator.model_name = model_name\n",
    "        config = temp_generator.get_model_specific_config()\n",
    "        \n",
    "        configs.append({\n",
    "            'Model': model_name,\n",
    "            'Patch Size': config['patch_size'],\n",
    "            'GPU Batch': config['gpu_batch'],\n",
    "            'Attention Layers': config.get('attention_layers', 'N/A'),\n",
    "            'Target Hint': config.get('target_layer_hint', 'Auto-resolve')\n",
    "        })\n",
    "    \n",
    "    # Print as table\n",
    "    print(f\"{'Model':<20} {'Patch Size':<12} {'GPU Batch':<12} {'Att. Layers':<12} {'Target Hint':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "    for config in configs:\n",
    "        print(f\"{config['Model']:<20} {config['Patch Size']:<12} {config['GPU Batch']:<12} {config['Attention Layers']:<12} {config['Target Hint']:<15}\")\n",
    "    \n",
    "    print(\"\\nConfiguration Details:\")\n",
    "    print(\"â€¢ Patch Size: Spatial resolution for CAM methods\")\n",
    "    print(\"â€¢ GPU Batch: Batch size for RISE method\")\n",
    "    print(\"â€¢ Att. Layers: Number of attention layers for ViT models\")\n",
    "    print(\"â€¢ Target Hint: Architecture-specific target layer guidance\")\n",
    "\n",
    "print_model_config_summary()\n",
    "\n",
    "# Execute the heatmap generation\n",
    "print(\"\\n=== Starting Heatmap Generation ===\")\n",
    "heatmap_results = generate_comprehensive_heatmaps(num_samples=2)  # Start with 2 samples per task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae1da17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "print(\"\\n=== Creating Comprehensive Visualization ===\")\n",
    "\n",
    "# Create the main heatmap grid\n",
    "output_dir = Path(\"./heatmap_results\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "main_grid_path = output_dir / \"comprehensive_heatmap_grid.png\"\n",
    "fig = create_heatmap_grid(heatmap_results, save_path=main_grid_path)\n",
    "\n",
    "print(f\"Main visualization saved to: {main_grid_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2512c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create individual task-specific visualizations\n",
    "def create_task_specific_grids(results_dict, output_dir):\n",
    "    \"\"\"Create separate visualizations for each task\"\"\"\n",
    "    \n",
    "    for task in results_dict.keys():\n",
    "        print(f\"Creating visualization for {task}...\")\n",
    "        \n",
    "        # Create task-specific results dict\n",
    "        task_results = {task: results_dict[task]}\n",
    "        \n",
    "        # Create visualization\n",
    "        task_grid_path = output_dir / f\"{task}_heatmap_grid.png\"\n",
    "        fig = create_heatmap_grid(task_results, save_path=task_grid_path)\n",
    "        \n",
    "        plt.close(fig)  # Close to save memory\n",
    "        print(f\"{task} visualization saved to: {task_grid_path}\")\n",
    "\n",
    "# Generate task-specific grids\n",
    "create_task_specific_grids(heatmap_results, output_dir)\n",
    "\n",
    "print(\"\\n=== Heatmap Generation Complete ===\")\n",
    "print(f\"All results saved in: {output_dir}\")\n",
    "print(\"Generated files:\")\n",
    "for file_path in output_dir.glob(\"*.png\"):\n",
    "    print(f\"  - {file_path.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8359d593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and Analysis\n",
    "def print_heatmap_summary(results_dict):\n",
    "    \"\"\"Print a summary of generated heatmaps\"\"\"\n",
    "    \n",
    "    print(\"\\n=== HEATMAP GENERATION SUMMARY ===\")\n",
    "    \n",
    "    total_combinations = 0\n",
    "    successful_combinations = 0\n",
    "    method_success_count = {'GradCAM': 0, 'ScoreCAM': 0, 'RISE': 0, 'Attention': 0}\n",
    "    \n",
    "    for task in results_dict.keys():\n",
    "        print(f\"\\nTask: {task}\")\n",
    "        \n",
    "        for model in results_dict[task].keys():\n",
    "            model_results = results_dict[task][model]\n",
    "            n_samples = len(model_results['images'])\n",
    "            \n",
    "            print(f\"  Model: {model} ({n_samples} samples)\")\n",
    "            \n",
    "            # Count successful heatmaps for each method\n",
    "            for sample_idx, heatmaps in enumerate(model_results['heatmaps']):\n",
    "                total_combinations += 1\n",
    "                \n",
    "                if heatmaps:  # If any heatmaps were generated\n",
    "                    successful_combinations += 1\n",
    "                \n",
    "                for method in ['GradCAM', 'ScoreCAM', 'RISE', 'Attention']:\n",
    "                    if method in heatmaps and heatmaps[method] is not None:\n",
    "                        method_success_count[method] += 1\n",
    "                        print(f\"    Sample {sample_idx+1}: {method} âœ“\")\n",
    "                    else:\n",
    "                        print(f\"    Sample {sample_idx+1}: {method} âœ—\")\n",
    "    \n",
    "    print(f\"\\n=== OVERALL STATISTICS ===\")\n",
    "    print(f\"Total task-model-sample combinations: {total_combinations}\")\n",
    "    print(f\"Successful combinations: {successful_combinations}\")\n",
    "    print(f\"Success rate: {successful_combinations/total_combinations*100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\n=== METHOD SUCCESS RATES ===\")\n",
    "    for method, count in method_success_count.items():\n",
    "        success_rate = count / total_combinations * 100 if total_combinations > 0 else 0\n",
    "        print(f\"{method}: {count}/{total_combinations} ({success_rate:.1f}%)\")\n",
    "\n",
    "# Print summary\n",
    "print_heatmap_summary(heatmap_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
