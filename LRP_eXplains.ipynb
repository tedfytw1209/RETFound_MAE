{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae19951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/blue/guoj1/tienyuchang/.conda/envs/octxai/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/blue/guoj1/tienyuchang/.conda/envs/octxai/lib/python3.9/site-packages/lxt/efficient/zennit_patches.py:22: UserWarning: 'zennit' library is not available. Please install it to use for vision transformers.\n",
      "  warn(\"'zennit' library is not available. Please install it to use for vision transformers.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x14aae40c5bb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from timm.models.layers import trunc_normal_\n",
    "from timm.data.mixup import Mixup\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
    "from transformers import (\n",
    "    ViTImageProcessor, ViTForImageClassification,\n",
    "    AutoImageProcessor, EfficientNetForImageClassification,\n",
    "    ResNetForImageClassification\n",
    ")\n",
    "\n",
    "import models_vit as models\n",
    "import util.lr_decay as lrd\n",
    "import util.misc as misc\n",
    "from util.datasets import build_dataset,DistributedSamplerWrapper,TransformWrapper\n",
    "from util.pos_embed import interpolate_pos_embed\n",
    "from util.misc import NativeScalerWithGradNormCount as NativeScaler\n",
    "from util.losses import FocalLoss, compute_alpha_from_labels\n",
    "from util.evaluation import InsertionMetric, DeletionMetric\n",
    "from baselines.Attention import Attention_Map\n",
    "from baselines.GradCAM import GradCAM\n",
    "from baselines.RISE import RISE, RISEBatch\n",
    "from huggingface_hub import hf_hub_download, login\n",
    "from engine_finetune import evaluate_half3D, train_one_epoch, evaluate\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from zennit.image import imgify\n",
    "from zennit.composites import LayerMapComposite\n",
    "import zennit.rules as z_rules\n",
    "from lxt.efficient import monkey_patch, monkey_patch_zennit\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fba72b5-7d26-4653-8833-3284cb85fe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = None\n",
    "patch_size = None\n",
    "input_size = 224\n",
    "nb_classes = 2\n",
    "finetune = \"RETFound_mae_natureOCT\"\n",
    "resume = \"output_dir/Cataract_all_split-IRB2024_v4-all-RETFound_mae_natureOCT-OCT-bs16ep50lr5e-4optadamw-roc_auceval--/checkpoint-best.pth\"\n",
    "\n",
    "model = models.__dict__['RETFound_mae'](\n",
    "        img_size=input_size,\n",
    "        num_classes=nb_classes,\n",
    "        drop_path_rate=0.2,\n",
    "        global_pool=True,\n",
    "        )\n",
    "patch_size = 16\n",
    "print(f\"Downloading pre-trained weights from: {finetune}\")\n",
    "checkpoint_path = hf_hub_download(\n",
    "    repo_id=f'YukunZhou/{finetune}',\n",
    "    filename=f'{finetune}.pth',\n",
    ")\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "print(\"Load pre-trained checkpoint from: %s\" % args.finetune)\n",
    "if args.model!='RETFound_mae':\n",
    "    checkpoint_model = checkpoint['teacher']\n",
    "else:\n",
    "    checkpoint_model = checkpoint['model']\n",
    "checkpoint_model = {k.replace(\"backbone.\", \"\"): v for k, v in checkpoint_model.items()}\n",
    "checkpoint_model = {k.replace(\"mlp.w12.\", \"mlp.fc1.\"): v for k, v in checkpoint_model.items()}\n",
    "checkpoint_model = {k.replace(\"mlp.w3.\", \"mlp.fc2.\"): v for k, v in checkpoint_model.items()}\n",
    "state_dict = model.state_dict()\n",
    "for k in ['head.weight', 'head.bias']:\n",
    "    if k in checkpoint_model and checkpoint_model[k].shape != state_dict[k].shape:\n",
    "        print(f\"Removing key {k} from pretrained checkpoint\")\n",
    "        del checkpoint_model[k]\n",
    "# interpolate position embedding\n",
    "interpolate_pos_embed(model, checkpoint_model)\n",
    "# load pre-trained model\n",
    "msg = model.load_state_dict(checkpoint_model, strict=False)\n",
    "trunc_normal_(model.head.weight, std=2e-5)\n",
    "processor = None\n",
    "#Load fine-tune model\n",
    "checkpoint = torch.load(resume, map_location='cpu')\n",
    "if 'model' in checkpoint:\n",
    "    checkpoint_model = checkpoint['model']\n",
    "else:\n",
    "    checkpoint_model = checkpoint\n",
    "model.load_state_dict(checkpoint_model, strict=False)\n",
    "print(\"Resume checkpoint %s\" % resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0296f74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[name_list,feature]=get_feature(data_path,\n",
    "                chkpt_dir,\n",
    "                device,\n",
    "                arch=arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "925d3994",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the feature\n",
    "df_feature = pd.DataFrame(feature)\n",
    "df_imgname = pd.DataFrame(name_list)\n",
    "df_visualization = pd.concat([df_imgname,df_feature], axis=1)\n",
    "column_name_list = []\n",
    "\n",
    "for i in range(1024):\n",
    "    column_name_list.append(\"feature_{}\".format(i))\n",
    "df_visualization.columns = [\"name\"] + column_name_list\n",
    "df_visualization.to_csv(\"Feature.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdaf547-5aea-460f-95b1-b3db620c8a57",
   "metadata": {},
   "source": [
    "# LXT sample provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a9cec4-e557-4e5a-abc0-3e263bb538e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import itertools\n",
    "from PIL import Image\n",
    "from torchvision.models import vision_transformer\n",
    "\n",
    "from zennit.image import imgify\n",
    "from zennit.composites import LayerMapComposite\n",
    "import zennit.rules as z_rules\n",
    "\n",
    "from lxt.efficient import monkey_patch, monkey_patch_zennit\n",
    "\n",
    "# Modify the Vision Transformer module to compute Layer-wise Relevance Propagation (LRP)\n",
    "# in the backward pass. For ViTs, we utilize the LRP Gamma rule. It is implemented\n",
    "# inside the 'zennit' library. To make it compatible with LXT, we also monkey patch it. That's it.\n",
    "monkey_patch(vision_transformer, verbose=True)\n",
    "monkey_patch_zennit(verbose=True)\n",
    "\n",
    "\n",
    "def get_vit_imagenet(device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Load a pre-trained Vision Transformer (ViT) model with ImageNet weights.\n",
    "    \n",
    "    Parameters:\n",
    "    device (str): Device to load the model on ('cuda' or 'cpu')\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (model, weights) - The ViT model and its pre-trained weights\n",
    "    \"\"\"\n",
    "    weights =vision_transformer.ViT_B_16_Weights.IMAGENET1K_V1\n",
    "    model = vision_transformer.vit_b_16(weights=weights)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Deactivate gradients on parameters to save memory\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    return model, weights\n",
    "\n",
    "# Load the pre-trained ViT model\n",
    "model, weights = get_vit_imagenet()\n",
    "\n",
    "# Load and preprocess the input image\n",
    "image = Image.open('docs/source/_static/cat_dog.jpg').convert('RGB')\n",
    "input_tensor = weights.transforms()(image).unsqueeze(0).to(\"cuda\")\n",
    "\n",
    "# Store the generated heatmaps\n",
    "heatmaps = []\n",
    "\n",
    "# Experiment with different gamma values for Conv2d and Linear layers\n",
    "# Gamma is a hyperparameter in LRP that controls how much positive vs. negative\n",
    "# contributions are considered in the explanation\n",
    "for conv_gamma, lin_gamma in itertools.product([0.1, 0.25, 100], [0, 0.01, 0.05, 0.1, 1]):\n",
    "    input_tensor.grad = None  # Reset gradients\n",
    "    print(\"Gamma Conv2d:\", conv_gamma, \"Gamma Linear:\", lin_gamma)\n",
    "    \n",
    "    # Define rules for the Conv2d and Linear layers using 'zennit'\n",
    "    # LayerMapComposite maps specific layer types to specific LRP rule implementations\n",
    "    zennit_comp = LayerMapComposite([\n",
    "        (torch.nn.Conv2d, z_rules.Gamma(conv_gamma)),\n",
    "        (torch.nn.Linear, z_rules.Gamma(lin_gamma)),\n",
    "    ])\n",
    "    \n",
    "    # Register the composite rules with the model\n",
    "    zennit_comp.register(model)\n",
    "    \n",
    "    # Forward pass with gradient tracking enabled\n",
    "    y = model(input_tensor.requires_grad_())\n",
    "    \n",
    "    # Get the top 5 predictions\n",
    "    _, top5_classes = torch.topk(y, 5, dim=1)\n",
    "    top5_classes = top5_classes.squeeze(0).tolist()\n",
    "    \n",
    "    # Get the class labels\n",
    "    labels = weights.meta[\"categories\"]\n",
    "    top5_labels = [labels[class_idx] for class_idx in top5_classes]\n",
    "    \n",
    "    # Print the top 5 predictions and their labels\n",
    "    for i, class_idx in enumerate(top5_classes):\n",
    "        print(f'Top {i+1} predicted class: {class_idx}, label: {top5_labels[i]}')\n",
    "    \n",
    "    # Backward pass for the highest probability class\n",
    "    # This initiates the LRP computation through the network\n",
    "    y[0, top5_classes[0]].backward()\n",
    "    \n",
    "    # Remove the registered composite to prevent interference in future iterations\n",
    "    zennit_comp.remove()\n",
    "    \n",
    "    # Calculate the relevance by computing Gradient * Input\n",
    "    # This is the final step of LRP to get the pixel-wise explanation\n",
    "    heatmap = (input_tensor * input_tensor.grad).sum(1)\n",
    "    \n",
    "    # Normalize relevance between [-1, 1] for plotting\n",
    "    heatmap = heatmap / abs(heatmap).max()\n",
    "    \n",
    "    # Store the normalized heatmap\n",
    "    heatmaps.append(heatmap[0].detach().cpu().numpy())\n",
    "\n",
    "# Visualize all heatmaps in a grid (3Ã—5) and save to a file\n",
    "# vmin and vmax control the color mapping range\n",
    "imgify(heatmaps, vmin=-1, vmax=1, grid=(3, 5)).save('vit_heatmap.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68db922-c6c4-4126-a7c8-2dcd7ba699b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "test",
   "name": "common-cu121.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu121:m123"
  },
  "kernelspec": {
   "display_name": "octxai",
   "language": "python",
   "name": "octxai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
